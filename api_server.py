from msp_core import (
    run_msp_recommendation,
    run_msp_information_summary,
    run_msp_information_summary_claude,
    run_msp_information_summary_pplx,
    extract_msp_name,
    query_embed,
    collection,
    run_msp_news_summary_clova
)
from utils import fix_korean_encoding, map_group_to_category
from fastapi import File, UploadFile
from excel_upload_handler import evaluate_uploaded_excel, compute_category_scores_from_excel_data, summarize_answers_for_subcategories
from clova_router import Executor
from pydantic import BaseModel
from difflib import get_close_matches
import os
import datetime  # ADD THIS IMPORT
from dotenv import load_dotenv
load_dotenv()
from fastapi import FastAPI, HTTPException, Depends, Request
from fastapi.responses import StreamingResponse, JSONResponse
import io
import json

group_to_category_cache = {}

class RouterQuery(BaseModel):
    query: str
    chat_history: list = []
    advanced: bool = False  # NEW
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, RedirectResponse
import requests
from vector_writer import run_from_msp_name
from admin_protected import router as admin_router, manager

# Register user_loader at import time to avoid "Missing user_loader callback" error
@manager.user_loader()
def load_user(username: str):
    from admin_protected import User
    env_username = os.getenv("ADMIN_USERNAME")
    if username == env_username:
        return User(name=username)

import chromadb
from chromadb import PersistentClient

app = FastAPI()

from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # or restrict to your React app URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(admin_router)
print("ðŸ“¦ admin router included")

# Load ChromaDB
CHROMA_PATH = os.path.abspath("chroma_store")
client = PersistentClient(path=CHROMA_PATH)
collection = client.get_or_create_collection("msp_chunks")

def manual_category_calculation(grouped_data, total_avg):
    """Fallback manual calculation if the existing logic fails"""
    
    # Enhanced group-to-category mapping based on your existing patterns
    group_to_category_mapping = {
        # Human Resources (ì¸ì ì—­ëŸ‰)
        "AI ì „ë¬¸ ì¸ë ¥ êµ¬ì„±": "ì¸ì ì—­ëŸ‰",
        "í”„ë¡œì íŠ¸ ê²½í—˜ ë° ì„±ê³µ ì‚¬ë¡€": "ì¸ì ì—­ëŸ‰", 
        "ì§€ì†ì ì¸ êµìœ¡ ë° í•™ìŠµ": "ì¸ì ì—­ëŸ‰",
        "í”„ë¡œì íŠ¸ ê´€ë¦¬ ë° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜": "ì¸ì ì—­ëŸ‰",
        "AI ìœ¤ë¦¬ ë° ì±…ìž„ ì˜ì‹": "ì¸ì ì—­ëŸ‰",
        
        # AI Technology (AIê¸°ìˆ ì—­ëŸ‰)
        "AI ê¸°ìˆ  ì—°êµ¬ ëŠ¥ë ¥": "AIê¸°ìˆ ì—­ëŸ‰",
        "AI ëª¨ë¸ ê°œë°œ ëŠ¥ë ¥": "AIê¸°ìˆ ì—­ëŸ‰",
        "AI í”Œëž«í¼ ë° ì¸í”„ë¼ êµ¬ì¶• ëŠ¥ë ¥": "AIê¸°ìˆ ì—­ëŸ‰", 
        "ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ëŠ¥ë ¥": "AIê¸°ìˆ ì—­ëŸ‰",
        "AI ê¸°ìˆ ì˜ ìœµí•© ë° í™œìš© ëŠ¥ë ¥": "AIê¸°ìˆ ì—­ëŸ‰",
        "AI ê¸°ìˆ ì˜ íŠ¹í—ˆ ë° ì¸ì¦ ë³´ìœ  í˜„í™©": "AIê¸°ìˆ ì—­ëŸ‰",
        
        # Solution (ì†”ë£¨ì…˜ ì—­ëŸ‰)
        "ë‹¤ì–‘ì„± ë° ì „ë¬¸ì„±": "ì†”ë£¨ì…˜ ì—­ëŸ‰",
        "ì•ˆì •ì„±": "ì†”ë£¨ì…˜ ì—­ëŸ‰", 
        "í™•ìž¥ì„± ë° ìœ ì—°ì„±": "ì†”ë£¨ì…˜ ì—­ëŸ‰",
        "ì‚¬ìš©ìž íŽ¸ì˜ì„±": "ì†”ë£¨ì…˜ ì—­ëŸ‰",
        "ë³´ì•ˆì„±": "ì†”ë£¨ì…˜ ì—­ëŸ‰",
        "ê¸°ìˆ  ì§€ì› ë° ìœ ì§€ë³´ìˆ˜": "ì†”ë£¨ì…˜ ì—­ëŸ‰",
        "ì°¨ë³„ì„± ë° ê²½ìŸë ¥": "ì†”ë£¨ì…˜ ì—­ëŸ‰",
        "ê°œë°œ ë¡œë“œë§µ ë° í–¥í›„ ê³„íš": "ì†”ë£¨ì…˜ ì—­ëŸ‰"
    }
    
    # Aggregate scores by main category
    main_categories = ["ì¸ì ì—­ëŸ‰", "AIê¸°ìˆ ì—­ëŸ‰", "ì†”ë£¨ì…˜ ì—­ëŸ‰"]
    category_data = {cat: [] for cat in main_categories}
    
    for group_name, items in grouped_data.items():
        # Map group to main category
        main_category = group_to_category_mapping.get(group_name)
        
        # Fallback: keyword matching
        if not main_category:
            group_lower = group_name.lower()
            if any(keyword in group_lower for keyword in ["ì¸ë ¥", "êµìœ¡", "í•™ìŠµ", "ê´€ë¦¬", "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "ìœ¤ë¦¬", "í”„ë¡œì íŠ¸"]):
                main_category = "ì¸ì ì—­ëŸ‰"
            elif any(keyword in group_lower for keyword in ["ai", "ê¸°ìˆ ", "ëª¨ë¸", "í”Œëž«í¼", "ì¸í”„ë¼", "ë°ì´í„°", "ìœµí•©", "íŠ¹í—ˆ", "ì—°êµ¬"]):
                main_category = "AIê¸°ìˆ ì—­ëŸ‰"
            elif any(keyword in group_lower for keyword in ["ì†”ë£¨ì…˜", "ë‹¤ì–‘ì„±", "ì•ˆì •ì„±", "í™•ìž¥ì„±", "íŽ¸ì˜ì„±", "ë³´ì•ˆ", "ì§€ì›", "ì°¨ë³„ì„±", "ë¡œë“œë§µ"]):
                main_category = "ì†”ë£¨ì…˜ ì—­ëŸ‰"
        
        # Add scores to appropriate category
        if main_category and main_category in category_data:
            for item in items:
                category_data[main_category].append(item["score"])
    
    # Calculate averages
    category_scores = {}
    for cat in main_categories:
        if category_data[cat]:
            category_scores[cat] = round(sum(category_data[cat]) / len(category_data[cat]), 2)
        else:
            # Use total average as fallback
            category_scores[cat] = total_avg
    
    return category_scores

def calculate_msp_category_scores(msp_name: str):
    """
    Calculate category scores for a specific MSP using the same logic as the upload summary
    """
    try:
        # Get all data for this MSP
        results = collection.get(
            where={"msp_name": msp_name},
            include=["metadatas"]
        )
        
        if not results["metadatas"]:
            return {"total_score": 0, "category_scores": {cat: 0 for cat in ["ì¸ì ì—­ëŸ‰", "AIê¸°ìˆ ì—­ëŸ‰", "ì†”ë£¨ì…˜ ì—­ëŸ‰"]}}
        
        # Organize data by group (similar to your upload logic)
        grouped_data = {}
        all_scores = []
        
        for meta in results["metadatas"]:
            question = meta.get("question", "")
            answer = meta.get("answer", "")
            score = meta.get("score")
            group = meta.get("group", "Unknown").strip()
            
            if not question or not answer or score is None:
                continue
                
            all_scores.append(score)
            
            if group not in grouped_data:
                grouped_data[group] = []
            
            grouped_data[group].append({
                "question": question,
                "answer": answer,
                "score": score
            })
        
        # Calculate total average
        total_avg = sum(all_scores) / len(all_scores) if all_scores else 0
        
        # Calculate category scores using your group-to-category mapping logic
        main_categories = ["ì¸ì ì—­ëŸ‰", "AIê¸°ìˆ ì—­ëŸ‰", "ì†”ë£¨ì…˜ ì—­ëŸ‰"]
        category_scores = {}
        
        try:
            # Use manual calculation since we're dealing with ChromaDB data
            category_scores = manual_category_calculation(grouped_data, total_avg)
        except Exception as e:
            print(f"[WARNING] Manual calculation failed for {msp_name}: {e}")
            # Final fallback: use total average for all categories
            for cat in main_categories:
                category_scores[cat] = total_avg
        
        # Ensure all main categories are present
        for cat in main_categories:
            if cat not in category_scores:
                category_scores[cat] = total_avg
        
        return {
            "total_score": round(total_avg, 2),
            "category_scores": category_scores
        }
        
    except Exception as e:
        print(f"[ERROR] Failed to calculate scores for {msp_name}: {e}")
        return {"total_score": 0, "category_scores": {cat: 0 for cat in ["ì¸ì ì—­ëŸ‰", "AIê¸°ìˆ ì—­ëŸ‰", "ì†”ë£¨ì…˜ ì—­ëŸ‰"]}}

# NOW ALL THE ENDPOINTS

@app.post("/run/{msp_name}")
def run_msp_vector_pipeline(msp_name: str):
    try:
        run_from_msp_name(msp_name)
        return {"message": f"Vector DB update completed for {msp_name}"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# Serve static files
app.mount("/static", StaticFiles(directory="static"), name="static")

@app.get("/ui")
def serve_ui(request: Request):
    try:
        user = manager(request)
        return FileResponse("static/vector-db-viewer.html")  # Vector DB viewer page
    except:
        return RedirectResponse(url="/login?next=/ui")

# Serve query UI
@app.get("/query")
def serve_query_ui():
    return FileResponse("static/query.html")

@app.get("/ui/data")
def get_filtered_chunks(question: str = None, min_score: int = 0):
    # Return flat format for public UI compatibility
    results = collection.get(include=["metadatas"])
    data = []
    for meta in results["metadatas"]:
        if not isinstance(meta.get("answer"), str) or not meta["answer"].strip():
            continue
        if question and question != meta["question"]:
            continue
        if meta["score"] is not None and int(meta["score"]) >= min_score:
            data.append({
                "msp_name": meta["msp_name"],
                "question": meta["question"],
                "score": meta["score"],
                "answer": meta["answer"]
            })
    return JSONResponse(content=data)

# Flat data endpoint for public UI
@app.get("/ui/data_flat")
def get_flat_chunks(question: str = None, min_score: int = 0):
    results = collection.get(include=["metadatas"])
    data = []
    for meta in results["metadatas"]:
        if not isinstance(meta.get("answer"), str) or not meta["answer"].strip():
            continue
        if question and question != meta["question"]:
            continue
        if meta["score"] is not None and int(meta["score"]) >= min_score:
            data.append({
                "msp_name": meta["msp_name"],
                "question": meta["question"],
                "score": meta["score"],
                "answer": meta["answer"]
            })
    return JSONResponse(content=data)

# Query/Ask endpoint
@app.post("/query/ask")
async def ask_question(request: Request):
    body = await request.json()
    question = body.get("question")
    min_score = int(body.get("min_score", 0))
    if not question:
        raise HTTPException(status_code=400, detail="Missing question")

    return run_msp_recommendation(question, min_score)

# Router endpoint
@app.post("/query/router")
async def query_router(data: RouterQuery):
    print(f"ðŸŸ¢ Advanced toggle received: {data.advanced}")
    executor = Executor()
    request_data = {
        "query": data.query,
        "chatHistory": data.chat_history
    }
    raw_result = executor.execute(request_data)

    import json
    import traceback

    try:
        if isinstance(raw_result, str):
            result = json.loads(raw_result)
        else:
            result = raw_result

        domain_result = result.get("domain", {}).get("result")
        blocked = result.get("blockedContent", {}).get("result", [])

        if domain_result == "mspevaluator":
            extracted_name = extract_msp_name(data.query)
            print(f"ðŸ§  CLOVA ì¶”ì¶œ íšŒì‚¬ëª…: {extracted_name}")
            print(f"ðŸŸ¢ Advanced toggle received: {data.advanced}")
            if "Information" in blocked:
                if data.advanced:
                    return run_msp_information_summary_pplx(data.query)
                else:
                    return run_msp_information_summary_claude(data.query)
            elif "Recommend" in blocked:
                return run_msp_recommendation(data.query, min_score=0)
            elif "Unrelated" in blocked:
                return {"answer": "ë³¸ ì‹œìŠ¤í…œì€ MSP í‰ê°€ ë„êµ¬ìž…ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì€ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ìž…ë ¥í•´ ì£¼ì„¸ìš”."}
            else:
                return {"answer": "ì§ˆë¬¸ ì˜ë„ë¥¼ ì •í™•ížˆ ë¶„ë¥˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."}
        else:
            return {"answer": "ë„ë©”ì¸ ë¶„ë¥˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."}
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Router ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# Advanced Naver route
@app.post("/query/advanced_naver")
async def query_advanced_naver(data: RouterQuery):
    return run_msp_news_summary_clova(data.query)

# Add protected /admin route using same login logic as /ui
@app.get("/admin")
def serve_admin_ui(request: Request):
    try:
        user = manager(request)
        return FileResponse("static/admin.html")
    except Exception as e:
        return RedirectResponse(url="/login?next=/admin")

# Serve main page at root
@app.get("/")
def serve_main_page():
    return FileResponse("static/main.html")

# Serve upload page
from fastapi.templating import Jinja2Templates
from fastapi.responses import HTMLResponse
templates = Jinja2Templates(directory="templates")

@app.get("/upload", response_class=HTMLResponse)
async def serve_upload_page(request: Request):
    return templates.TemplateResponse("upload.html", {"request": request})

# Excel upload endpoint
from fastapi import UploadFile, File

@app.post("/api/upload_excel")
async def upload_excel(file: UploadFile = File(...)):
    try:
        from excel_upload_handler import evaluate_uploaded_excel, compute_category_scores_from_excel_data

        evaluated = evaluate_uploaded_excel(file)
        summary_df = compute_category_scores_from_excel_data(evaluated)

        print(f"[DEBUG] summary_df.columns: {summary_df.columns.tolist()}")

        flat_results = []
        skipped_items = []
        for category_name, qa_list in evaluated.items():
            if not isinstance(qa_list, list):
                skipped_items.append({
                    "category": category_name,
                    "item": qa_list,
                    "reason": f"Expected a list but got: {type(qa_list)}"
                })
                continue
            if category_name == "summary":
                continue
            for item in qa_list:
                if (
                    not isinstance(item, dict)
                    or not item.get("question")
                    or not item.get("answer")
                    or item.get("score") is None
                ):
                    skipped_items.append({
                        "category": category_name,
                        "item": item,
                        "reason": "Missing required keys, empty fields, or invalid format"
                    })
                    continue
                flat_results.append({
                    "category": category_name,
                    "question": item["question"],
                    "answer": item["answer"],
                    "score": item["score"],
                    "group": item.get("group")  # Preserve group info for summary
                })

        from collections import defaultdict
        # Build group to question count mapping, with group key stripped
        group_to_questions = defaultdict(list)
        for q in flat_results:
            group_key = q["group"]
            if isinstance(group_key, str):
                group_key = group_key.strip()
            group_to_questions[group_key].append(q)

        group_question_counts = {group: len(questions) for group, questions in group_to_questions.items()}

        # Filter from summary_df instead of recomputing
        group_summary = []
        for record in summary_df.to_dict(orient="records"):
            print(f"[DEBUG] Summary Record: {record}")
            name = record.get("Category")
            score = record.get("Score")
            # Ensure name is a string and strip whitespace
            if not isinstance(name, str):
                continue
            name = name.strip()
            print(f"[DEBUG] âž• Group Summary Item Candidate - name: {name}, score: {score}")
            if "ì´ì " in name:
                continue
            try:
                score = float(score)
            except (ValueError, TypeError):
                continue
            group_summary.append({
                "name": name,
                "score": score,
                "questions": group_question_counts.get(name, 0)
            })
            print(f"[DEBUG] Added group summary item: name={name}, score={score}")

        print(f"[DEBUG] Final Group Summary Payload: {group_summary}")

        group_to_category = {}

        # The category_name in evaluated dict comes from sheet names
        for category_name, qa_list in evaluated.items():
            if not isinstance(qa_list, list):
                continue
            if category_name == "summary":
                continue
            
            # The category_name here is the parent category (sheet name)
            parent_category = category_name
            
            # Map all groups found in this sheet to the parent category
            seen_groups = set()
            for item in qa_list:
                group = item.get("group")
                if group and isinstance(group, str):
                    group = group.strip()
                    if group not in seen_groups:
                        group_to_category[group] = parent_category
                        seen_groups.add(group)
            
            # Also map the parent category to itself
            group_to_category[parent_category] = parent_category

        print(f"[DEBUG] Dynamic group_to_category mapping: {json.dumps(group_to_category, ensure_ascii=False, indent=2)}")

        return JSONResponse(content={
            "evaluated_questions": flat_results,
            "summary": summary_df.to_dict(orient="records"),
            "skipped_items": skipped_items,
            "groups": group_summary,  # THIS IS ALREADY HERE - just make sure it's used
            "group_to_category": group_to_category
        })
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Excel í‰ê°€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# New endpoint: /api/get_summary
from fastapi import Request
@app.post("/api/get_summary")
async def get_summary(request: Request):
    try:
        data = await request.json()
        evaluated = data.get("evaluated")
        if evaluated is None:
            raise HTTPException(status_code=400, detail="Missing 'evaluated' data")

        # Debug: print evaluated
        print(f"[DEBUG] Received evaluated (len={len(evaluated)}): {evaluated}")

        # Group by group field (fallback to category if missing)
        grouped = {}
        for item in evaluated:
            group = item.get("group") or item.get("category") or "Unknown"
            grouped.setdefault(group, []).append(item)

        # Debug: print grouped keys
        print(f"[DEBUG] Grouped keys: {list(grouped.keys())}")

        from excel_upload_handler import summarize_answers_for_subcategories
        # Debug: entering summarize
        print("[DEBUG] Entering summarize_answers_for_subcategories")
        summary = summarize_answers_for_subcategories(grouped)
        return JSONResponse(content={"subcategory_summaries": summary})
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Summary processing failed: {str(e)}")

@app.post("/api/add_to_vector_db")
async def add_to_vector_db(data: dict):
    try:
        from vector_writer import run_from_direct_input
        msp_name = data.get("msp_name")
        if not msp_name:
            raise HTTPException(status_code=400, detail="Missing msp_name")

        items = data.get("items", [])
        if not isinstance(items, list):
            raise HTTPException(status_code=400, detail="Missing or invalid items list")

        summary = data.get("summary")
        if summary is None:
            raise HTTPException(status_code=400, detail="Missing summary")

        run_from_direct_input(msp_name, items, summary)
        return {"message": f"Successfully added {len(items)} items to vector DB for {msp_name}"}
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Vector DB update failed: {str(e)}")

@app.get("/api/get_radar_data")
async def get_radar_data():
    try:
        # Return empty data since we're not using global variable anymore
        # Frontend should use the data from upload response instead
        return JSONResponse(content={})
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Radar data failed: {str(e)}")

@app.get("/api/get_group_to_category_map")
async def get_group_to_category_map():
    try:
        # Directly return the cached mapping built during upload
        from fastapi.responses import JSONResponse
        global group_to_category_cache
        if not group_to_category_cache:
            raise HTTPException(status_code=404, detail="Group-to-category map not available yet. Upload data first.")
        return JSONResponse(content=group_to_category_cache)
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Group-to-category map failed: {str(e)}")

@app.get("/api/leaderboard")
async def get_leaderboard():
    try:
        # Get all unique MSP names
        results = collection.get(include=["metadatas"])
        msp_names = set()
        
        for meta in results["metadatas"]:
            msp_name = meta.get("msp_name")
            if msp_name:
                msp_names.add(msp_name)
        
        print(f"[DEBUG] Found {len(msp_names)} unique MSPs: {list(msp_names)}")
        
        # Calculate scores for each MSP using the unified logic
        leaderboard = []
        for msp_name in msp_names:
            print(f"[DEBUG] Calculating scores for: {msp_name}")
            scores = calculate_msp_category_scores(msp_name)
            
            leaderboard.append({
                "name": msp_name,
                "total_score": scores["total_score"],
                "category_scores": scores["category_scores"]
            })
            
            print(f"[DEBUG] {msp_name}: Total={scores['total_score']}, Categories={scores['category_scores']}")
        
        # Sort by total score (descending)
        leaderboard.sort(key=lambda x: x["total_score"], reverse=True)
        
        return JSONResponse(content=leaderboard)

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Leaderboard generation failed: {str(e)}")

# Optional: Add a debug endpoint to test individual MSP calculation
@app.get("/api/debug_msp/{msp_name}")
async def debug_msp_calculation(msp_name: str):
    """Debug endpoint to see detailed calculation for a specific MSP"""
    try:
        # Get raw data
        results = collection.get(
            where={"msp_name": msp_name},
            include=["metadatas"]
        )
        
        # Calculate scores
        scores = calculate_msp_category_scores(msp_name)
        
        # Organize by group for debugging
        grouped_data = {}
        for meta in results["metadatas"]:
            group = meta.get("group", "Unknown")
            if group not in grouped_data:
                grouped_data[group] = []
            grouped_data[group].append({
                "question": meta.get("question", "")[:50] + "...",
                "score": meta.get("score")
            })
        
        return {
            "msp_name": msp_name,
            "calculated_scores": scores,
            "raw_groups": {k: len(v) for k, v in grouped_data.items()},
            "sample_data": {k: v[:2] for k, v in grouped_data.items()}  # First 2 items per group
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Debug failed: {str(e)}")

# Also add this debug endpoint to help you understand your current data structure:
@app.get("/api/debug_groups")
async def debug_groups():
    """Debug endpoint to see what groups/categories exist in your data"""
    try:
        results = collection.get(include=["metadatas"])
        
        groups_by_msp = {}
        all_groups = set()
        
        for meta in results["metadatas"]:
            msp_name = meta.get("msp_name")
            group = meta.get("group") or meta.get("category") or "Unknown"
            
            if msp_name:
                if msp_name not in groups_by_msp:
                    groups_by_msp[msp_name] = set()
                groups_by_msp[msp_name].add(group)
                all_groups.add(group)
        
        # Convert sets to lists for JSON serialization
        groups_by_msp = {k: list(v) for k, v in groups_by_msp.items()}
        
        return {
            "all_unique_groups": sorted(list(all_groups)),
            "groups_by_msp": groups_by_msp,
            "total_msps": len(groups_by_msp)
        }
    
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Debug failed: {str(e)}")

@app.post("/api/fix_existing_data")
async def fix_existing_data():
    """Fix encoding issues and add proper categories to existing data"""
    try:
        # Fixed ChromaDB API call - remove "ids" from include
        results = collection.get(include=["metadatas"])
        
        if not results["metadatas"]:
            return {"message": "No data found"}
        
        # Get IDs separately
        all_results = collection.get()
        ids = all_results["ids"]
        
        if len(ids) != len(results["metadatas"]):
            raise HTTPException(status_code=500, detail="Mismatch between IDs and metadata count")
        
        updates = []
        msp_names = set()
        
        # First pass: collect MSP names and fix encoding
        for i, meta in enumerate(results["metadatas"]):
            msp_name = meta.get("msp_name", "")
            
            # Try to fix MSP name encoding
            if msp_name:
                try:
                    # Try different decoding approaches
                    for encoding in ['utf-8', 'euc-kr', 'cp949']:
                        try:
                            if isinstance(msp_name, str) and 'ï¿½' in msp_name:
                                # Try to fix corrupted encoding
                                bytes_data = msp_name.encode('iso-8859-1')
                                fixed_name = bytes_data.decode(encoding)
                                msp_name = fixed_name
                                break
                        except:
                            continue
                except:
                    pass
            
            msp_names.add(msp_name)
            
            # Fix other text fields
            from utils import fix_korean_encoding, map_group_to_category
            question = fix_korean_encoding(str(meta.get("question", "")))
            answer = fix_korean_encoding(str(meta.get("answer", "")))
            group = fix_korean_encoding(str(meta.get("group", "")))
            
            # Determine proper category based on the question content since groups are "Unknown"
            # We'll try to infer category from the question itself
            category = infer_category_from_question(question)
            group_name = infer_group_from_question(question)
            
            # Create updated metadata
            updated_meta = {
                "msp_name": msp_name,
                "question": question,
                "answer": answer,
                "score": meta.get("score", 0),
                "group": group_name,  # Use inferred group
                "category": category,
                "timestamp": meta.get("timestamp", datetime.datetime.now(datetime.timezone.utc).isoformat())
            }
            
            updates.append({
                "id": ids[i],
                "metadata": updated_meta
            })
        
        print(f"[DEBUG] Found MSP names: {msp_names}")
        print(f"[DEBUG] Prepared {len(updates)} updates")
        
        # Apply updates in batches
        batch_size = 50  # Smaller batch size to avoid issues
        updated_count = 0
        
        for i in range(0, len(updates), batch_size):
            batch = updates[i:i + batch_size]
            
            try:
                batch_ids = [item["id"] for item in batch]
                batch_metadatas = [item["metadata"] for item in batch]
                
                collection.update(ids=batch_ids, metadatas=batch_metadatas)
                updated_count += len(batch)
                print(f"[DEBUG] Updated batch {i//batch_size + 1}: {len(batch)} items")
                
            except Exception as e:
                print(f"[ERROR] Failed to update batch {i//batch_size + 1}: {e}")
                # Try updating one by one for this batch
                for item in batch:
                    try:
                        collection.update(ids=[item["id"]], metadatas=[item["metadata"]])
                        updated_count += 1
                    except Exception as e2:
                        print(f"[ERROR] Failed to update individual item: {e2}")
        
        return {
            "message": f"Updated {updated_count} entries",
            "msp_names_found": list(msp_names),
            "total_entries": len(results["metadatas"])
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Fix failed: {str(e)}")

def infer_category_from_question(question):
    """Infer category from question content since groups are Unknown"""
    if not question:
        return "ë¯¸ë¶„ë¥˜"
    
    question_lower = question.lower()
    
    # Human resources patterns
    if any(keyword in question_lower for keyword in [
        "ì¸ë ¥", "ì§ì›", "êµìœ¡", "í•™ìŠµ", "í”„ë¡œì íŠ¸ ê²½í—˜", "ì„±ê³µ ì‚¬ë¡€", 
        "ê´€ë¦¬", "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜", "ìœ¤ë¦¬", "ì±…ìž„", "ì „ë¬¸", "êµ¬ì„±"
    ]):
        return "ì¸ì ì—­ëŸ‰"
    
    # AI technology patterns
    elif any(keyword in question_lower for keyword in [
        "ai", "ê¸°ìˆ ", "ëª¨ë¸", "í”Œëž«í¼", "ì¸í”„ë¼", "ë°ì´í„°", "ìœµí•©", 
        "íŠ¹í—ˆ", "ì—°êµ¬", "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "ìžì—°ì–´", "ê°œë°œ"
    ]):
        return "AIê¸°ìˆ ì—­ëŸ‰"
    
    # Solution patterns
    elif any(keyword in question_lower for keyword in [
        "ì†”ë£¨ì…˜", "ë‹¤ì–‘ì„±", "ì•ˆì •ì„±", "í™•ìž¥ì„±", "íŽ¸ì˜ì„±", "ë³´ì•ˆ", 
        "ì§€ì›", "ì°¨ë³„ì„±", "ë¡œë“œë§µ", "ê³„íš", "ìœ ì§€ë³´ìˆ˜"
    ]):
        return "ì†”ë£¨ì…˜ ì—­ëŸ‰"
    
    return "ë¯¸ë¶„ë¥˜"


def infer_group_from_question(question):
    """Infer more specific group from question content"""
    if not question:
        return "ë¯¸ë¶„ë¥˜"
    
    question_lower = question.lower()
    
    # Specific group patterns based on common question types
    if "ì¸ë ¥" in question_lower and ("êµ¬ì„±" in question_lower or "ë¹„ìœ¨" in question_lower):
        return "AI ì „ë¬¸ ì¸ë ¥ êµ¬ì„±"
    elif "í”„ë¡œì íŠ¸" in question_lower and ("ê²½í—˜" in question_lower or "ì‚¬ë¡€" in question_lower):
        return "í”„ë¡œì íŠ¸ ê²½í—˜ ë° ì„±ê³µ ì‚¬ë¡€"
    elif "êµìœ¡" in question_lower or "í•™ìŠµ" in question_lower:
        return "ì§€ì†ì ì¸ êµìœ¡ ë° í•™ìŠµ"
    elif "ê´€ë¦¬" in question_lower or "ì»¤ë®¤ë‹ˆì¼€ì´ì…˜" in question_lower:
        return "í”„ë¡œì íŠ¸ ê´€ë¦¬ ë° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜"
    elif "ìœ¤ë¦¬" in question_lower or "ì±…ìž„" in question_lower:
        return "AI ìœ¤ë¦¬ ë° ì±…ìž„ ì˜ì‹"
    elif "ì—°êµ¬" in question_lower:
        return "AI ê¸°ìˆ  ì—°êµ¬ ëŠ¥ë ¥"
    elif "ëª¨ë¸" in question_lower and "ê°œë°œ" in question_lower:
        return "AI ëª¨ë¸ ê°œë°œ ëŠ¥ë ¥"
    elif "í”Œëž«í¼" in question_lower or "ì¸í”„ë¼" in question_lower:
        return "AI í”Œëž«í¼ ë° ì¸í”„ë¼ êµ¬ì¶• ëŠ¥ë ¥"
    elif "ë°ì´í„°" in question_lower and ("ì²˜ë¦¬" in question_lower or "ë¶„ì„" in question_lower):
        return "ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ëŠ¥ë ¥"
    elif "ìœµí•©" in question_lower or "í™œìš©" in question_lower:
        return "AI ê¸°ìˆ ì˜ ìœµí•© ë° í™œìš© ëŠ¥ë ¥"
    elif "íŠ¹í—ˆ" in question_lower or "ì¸ì¦" in question_lower:
        return "AI ê¸°ìˆ ì˜ íŠ¹í—ˆ ë° ì¸ì¦ ë³´ìœ  í˜„í™©"
    elif "ë‹¤ì–‘ì„±" in question_lower or "ì „ë¬¸ì„±" in question_lower:
        return "ë‹¤ì–‘ì„± ë° ì „ë¬¸ì„±"
    elif "ì•ˆì •ì„±" in question_lower:
        return "ì•ˆì •ì„±"
    elif "í™•ìž¥ì„±" in question_lower or "ìœ ì—°ì„±" in question_lower:
        return "í™•ìž¥ì„± ë° ìœ ì—°ì„±"
    elif "íŽ¸ì˜ì„±" in question_lower or "ì‚¬ìš©ìž" in question_lower:
        return "ì‚¬ìš©ìž íŽ¸ì˜ì„±"
    elif "ë³´ì•ˆ" in question_lower:
        return "ë³´ì•ˆì„±"
    elif "ì§€ì›" in question_lower or "ìœ ì§€ë³´ìˆ˜" in question_lower:
        return "ê¸°ìˆ  ì§€ì› ë° ìœ ì§€ë³´ìˆ˜"
    elif "ì°¨ë³„ì„±" in question_lower or "ê²½ìŸë ¥" in question_lower:
        return "ì°¨ë³„ì„± ë° ê²½ìŸë ¥"
    elif "ë¡œë“œë§µ" in question_lower or "ê³„íš" in question_lower:
        return "ê°œë°œ ë¡œë“œë§µ ë° í–¥í›„ ê³„íš"
    
    # If no specific match, return category-level group
    category = infer_category_from_question(question)
    return category if category != "ë¯¸ë¶„ë¥˜" else "ê¸°íƒ€"

@app.post("/api/refresh_leaderboard_public")
async def refresh_leaderboard_public():
    """Public endpoint to refresh leaderboard data when needed"""
    try:
        # Get all results
        results = collection.get(include=["metadatas"])
        
        if not results["metadatas"]:
            return {"success": False, "message": "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # Get IDs separately
        all_results = collection.get()
        ids = all_results["ids"]
        
        # Count how many items need updating
        needs_update = 0
        for meta in results["metadatas"]:
            current_group = meta.get("group", "")
            current_category = meta.get("category", "")
            if current_group in ["Unknown", "unknown", ""] or not current_category:
                needs_update += 1
        
        # If no updates needed, return early
        if needs_update == 0:
            return {
                "success": True, 
                "message": "ìˆœìœ„í‘œê°€ ì´ë¯¸ ìµœì‹  ìƒíƒœìž…ë‹ˆë‹¤",
                "updated_count": 0,
                "total_checked": len(results["metadatas"])
            }
        
        updates = []
        
        # Process each entry that needs updating
        for i, meta in enumerate(results["metadatas"]):
            current_group = meta.get("group", "")
            current_category = meta.get("category", "")
            
            if current_group in ["Unknown", "unknown", ""] or not current_category:
                from utils import fix_korean_encoding
                
                msp_name = meta.get("msp_name", "")
                question = fix_korean_encoding(str(meta.get("question", "")))
                answer = fix_korean_encoding(str(meta.get("answer", "")))
                
                # Infer proper group and category
                category = infer_category_from_question(question)
                group_name = infer_group_from_question(question)
                
                updated_meta = {
                    "msp_name": msp_name,
                    "question": question,
                    "answer": answer,
                    "score": meta.get("score", 0),
                    "group": group_name,
                    "category": category,
                    "timestamp": meta.get("timestamp", datetime.datetime.now(datetime.timezone.utc).isoformat())
                }
                
                updates.append({
                    "id": ids[i],
                    "metadata": updated_meta
                })
        
        # Apply updates
        updated_count = 0
        if updates:
            batch_size = 50
            for i in range(0, len(updates), batch_size):
                batch = updates[i:i + batch_size]
                try:
                    batch_ids = [item["id"] for item in batch]
                    batch_metadatas = [item["metadata"] for item in batch]
                    collection.update(ids=batch_ids, metadatas=batch_metadatas)
                    updated_count += len(batch)
                except Exception as e:
                    print(f"[ERROR] Batch update failed: {e}")
                    # Try individual updates for failed batch
                    for item in batch:
                        try:
                            collection.update(ids=[item["id"]], metadatas=[item["metadata"]])
                            updated_count += 1
                        except Exception as e2:
                            print(f"[ERROR] Individual update failed: {e2}")
        
        return {
            "success": True, 
            "message": f"ìˆœìœ„í‘œ ìƒˆë¡œê³ ì¹¨ ì™„ë£Œ: {updated_count}ê°œ í•­ëª© ì—…ë°ì´íŠ¸",
            "updated_count": updated_count,
            "total_checked": len(results["metadatas"])
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return {"success": False, "message": f"ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨: {str(e)}"}

# Also add a simple status check endpoint
@app.get("/api/leaderboard_status_public")
async def get_public_leaderboard_status():
    """Public endpoint to check if leaderboard needs refresh"""
    try:
        results = collection.get(include=["metadatas"])
        
        if not results["metadatas"]:
            return {"needs_refresh": False, "unknown_count": 0, "total": 0}
        
        # Count unknown groups
        unknown_count = 0
        for meta in results["metadatas"]:
            group = meta.get("group", "")
            category = meta.get("category", "")
            if group in ["Unknown", "unknown", ""] or not category:
                unknown_count += 1
        
        return {
            "needs_refresh": unknown_count > 0,
            "unknown_count": unknown_count,
            "total": len(results["metadatas"])
        }
        
    except Exception as e:
        return {"needs_refresh": False, "error": str(e)}

@app.get("/leaderboard")
def serve_leaderboard():
    return FileResponse("static/index.html")

@app.get("/leaderboard/{path:path}")
def serve_leaderboard_paths(path: str):
    return FileResponse("static/index.html")

@app.get("/search")
def serve_react_search():
    return FileResponse("static/search.html")
