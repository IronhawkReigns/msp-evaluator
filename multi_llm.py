from msp_core import query_embed, collection  # Reuse existing infrastructure
from fastapi import HTTPException
import anthropic
import os
from openai import OpenAI
from collections import defaultdict
import traceback

def run_multi_llm_msp_recommendation(question: str, min_score: int):
    """
    Enhanced MSP recommendation using multi-LLM interaction cycle:
    HCX Responder â†’ Claude Critic â†’ Claude Refiner
    """
    try:
        # Step 1: Collect and prepare data
        raw_data = collect_vector_data(question, min_score)
        if not raw_data["grouped_chunks"]:
            return {"answer": "í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” í‰ê°€ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
        
        # Step 2: Manage context and select companies
        context_data = manage_context_selection(raw_data["grouped_chunks"], question)
        
        # Step 3: HCX Responder (with fallback)
        hcx_result = call_hcx_responder(question, context_data["full_context"])
        
        # Step 4: Claude Critic
        critic_result = call_claude_critic(question, hcx_result["recommendation"], context_data["full_context"])
        
        # Step 5: Claude Refiner
        final_result = call_claude_refiner(question, hcx_result["recommendation"], 
                                         critic_result["analysis"], context_data["full_context"])
        
        # Step 6: Compile final response
        return compile_final_response(
            final_result["recommendation"],
            raw_data,
            context_data,
            hcx_result,
            critic_result,
            final_result
        )
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Multi-LLM recommendation failed: {str(e)}")


def collect_vector_data(question: str, min_score: int):
    """Collect and organize data from vector database"""
    from collections import defaultdict
    
    query_vector = query_embed(question)
    query_results = collection.query(
        query_embeddings=[query_vector],
        n_results=20
    )
    
    grouped_chunks = defaultdict(list)
    for meta in query_results["metadatas"][0]:
        if not isinstance(meta.get("answer"), str) or not meta["answer"].strip():
            continue
        if meta["score"] is not None and int(meta["score"]) >= min_score:
            grouped_chunks[meta["msp_name"]].append({
                "question": meta['question'],
                "answer": meta['answer'],
                "score": meta['score'],
                "category": meta.get('category', 'ë¯¸ë¶„ë¥˜'),
                "group": meta.get('group', 'ê¸°íƒ€')
            })

    # Calculate analytics for each company
    company_analytics = {}
    for msp, qa_list in grouped_chunks.items():
        scores = [qa['score'] for qa in qa_list]
        analytics = {
            'overall_avg': round(sum(scores) / len(scores), 2),
            'evidence_quality': len([qa for qa in qa_list if len(qa['answer']) > 150]),
            'total_responses': len(qa_list)
        }
        company_analytics[msp] = analytics

    return {
        "grouped_chunks": grouped_chunks,
        "company_analytics": company_analytics,
        "query_results": query_results
    }


def manage_context_selection(grouped_chunks: dict, question: str):
    """Manage context length by selecting top companies and formatting context"""
    
    # Determine context limit based on question complexity
    question_complexity_indicators = len([
        keyword for keyword in ['ëŒ€ë¹„', 'ë¹„êµ', 'vs', 'ê°•ì ', 'ì•½ì ', 'ì°¨ì´', 'ìš°ìœ„', 'ê²½ìŸ', 
                               'ë” ì¢‹ì€', 'ëŒ€ì•ˆ', 'ì˜µì…˜', 'ì„ íƒì§€', 'ì¶”ì²œ', 'ai', 'ë¨¸ì‹ ëŸ¬ë‹', 
                               'ì •ë¶€', 'ê³µê³µ', 'ëŒ€ê·œëª¨', 'ë©€í‹°']
        if keyword in question.lower()
    ])
    
    if question_complexity_indicators >= 3:
        max_companies = 7
    elif question_complexity_indicators >= 1:
        max_companies = 5
    else:
        max_companies = 4
    
    print(f"ğŸ§  Question complexity indicators: {question_complexity_indicators}, limiting to top {max_companies} companies")
    
    # Sort and select companies
    company_analytics = {msp: calculate_company_analytics(qa_list) for msp, qa_list in grouped_chunks.items()}
    company_list = [(msp, qa_list, company_analytics[msp]) for msp, qa_list in grouped_chunks.items()]
    company_list.sort(key=lambda x: x[2]['overall_avg'], reverse=True)
    selected_companies = company_list[:max_companies]
    
    # Format context
    full_context = format_company_context(selected_companies)
    
    # Track exclusions
    excluded_companies = [item[0] for item in company_list[max_companies:]]
    if excluded_companies:
        print(f"ğŸ“Š Context management: Selected top {max_companies} companies by avg score, excluded: {excluded_companies}")
    
    return {
        "full_context": full_context,
        "selected_companies": selected_companies,
        "excluded_companies": excluded_companies,
        "max_companies": max_companies,
        "context_stats": {
            "total_companies_available": len(grouped_chunks),
            "companies_included": len(selected_companies),
            "companies_excluded": excluded_companies,
            "context_length_chars": len(full_context),
            "selection_criteria": "overall_avg_score_descending"
        }
    }


def calculate_company_analytics(qa_list: list):
    """Calculate analytics for a single company"""
    scores = [qa['score'] for qa in qa_list]
    return {
        'overall_avg': round(sum(scores) / len(scores), 2),
        'evidence_quality': len([qa for qa in qa_list if len(qa['answer']) > 150]),
        'total_responses': len(qa_list)
    }


def format_company_context(selected_companies: list):
    """Format company data into context string for LLMs"""
    analysis_context = []
    
    for msp, qa_list, analytics in selected_companies:
        sorted_qa = sorted(qa_list, key=lambda x: (x['score'], len(x['answer'])), reverse=True)
        top_evidence = sorted_qa[:6]
        
        company_block = f"""
=== {msp} ë¶„ì„ ===
í‰ê· : {analytics['overall_avg']}/5ì  | ì‘ë‹µ ìˆ˜: {analytics['total_responses']}ê°œ
ìƒì„¸ ë‹µë³€: {analytics['evidence_quality']}ê°œ

í•µì‹¬ ê·¼ê±°:
{chr(10).join([f"[{qa['score']}ì ] {qa['category']} | Q: {qa['question'][:60]}{'...' if len(qa['question']) > 60 else ''}" + 
              f"{chr(10)}    A: {qa['answer'][:200]}{'...' if len(qa['answer']) > 200 else ''}"
              for qa in top_evidence])}
"""
        analysis_context.append(company_block)
    
    return "\n".join(analysis_context)


def call_hcx_responder(question: str, full_context: str):
    """Call HCX for initial recommendation with quality validation and fallback"""
    import os
    from openai import OpenAI
    
    hcx_prompt = f"""ë‹¹ì‹ ì€ MSP ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” íšŒì‚¬ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: "{question}"

{full_context}

=== ë¶„ì„ ì§€ì¹¨ ===
1. ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì§ì ‘ ê´€ë ¨ëœ ì—­ëŸ‰ì„ ê°€ì§„ íšŒì‚¬ë“¤ì„ ì‹ë³„
2. ê° íšŒì‚¬ì˜ í•´ë‹¹ ë¶„ì•¼ ê²½í—˜ê³¼ í‰ê°€ ì ìˆ˜ë¥¼ ì¢…í•© ë¶„ì„
3. ìƒìœ„ 2-3ê°œ íšŒì‚¬ë¥¼ ì„ ì •í•˜ê³  ìˆœìœ„ ê²°ì •
4. ê° ì¶”ì²œì˜ êµ¬ì²´ì  ê·¼ê±° ì œì‹œ

ì‘ë‹µ í˜•ì‹:
**ì¶”ì²œ ìˆœìœ„**
1ìˆœìœ„: [íšŒì‚¬ëª…] - [í•µì‹¬ ì„ ì • ì´ìœ ]
2ìˆœìœ„: [íšŒì‚¬ëª…] - [í•µì‹¬ ì„ ì • ì´ìœ ]

**ìƒì„¸ ê·¼ê±°**
ê° íšŒì‚¬ë³„ë¡œ ê´€ë ¨ í‰ê°€ ë‚´ìš©ê³¼ ì ìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì  ì„¤ëª…"""

    # Call HCX
    CLOVA_API_KEY = os.getenv("CLOVA_API_KEY_OPENAI")
    API_URL = "https://clovastudio.stream.ntruss.com/v1/openai"
    hcx_client = OpenAI(api_key=CLOVA_API_KEY, base_url=API_URL)
    
    hcx_response = hcx_client.chat.completions.create(
        model="HCX-005",
        messages=[
            {"role": "system", "content": "MSP í‰ê°€ ì „ë¬¸ê°€ë¡œì„œ ë°ì´í„° ê¸°ë°˜ì˜ ê°ê´€ì  ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤."},
            {"role": "user", "content": hcx_prompt}
        ],
        top_p=0.6,
        temperature=0.3,
        max_tokens=800
    )
    
    hcx_recommendation = hcx_response.choices[0].message.content.strip()
    print(f"âœ… HCX Response length: {len(hcx_recommendation)} chars")
    
    # Validate and potentially use fallback
    validation_result = validate_and_fallback_hcx(hcx_recommendation, question, hcx_client, full_context)
    
    return {
        "recommendation": validation_result["final_recommendation"],
        "quality_metrics": validation_result["quality_metrics"],
        "fallback_used": validation_result["fallback_used"]
    }


def validate_and_fallback_hcx(hcx_recommendation: str, question: str, hcx_client, full_context: str):
    """Validate HCX response quality and apply fallback if needed"""
    
    def validate_hcx_response(response_text, question):
        """Validate HCX response quality"""
        validation_issues = []
        
        # Check 1: Minimum length
        if len(response_text) < 200:
            validation_issues.append("response_too_short")
        
        # Check 2: Contains ranking/priority indicators
        ranking_indicators = ['1ìˆœìœ„', '2ìˆœìœ„', 'ì¶”ì²œ', 'ìˆœìœ„', '1ìœ„', '2ìœ„', 'ì²« ë²ˆì§¸', 'ë‘ ë²ˆì§¸']
        has_ranking = any(indicator in response_text for indicator in ranking_indicators)
        if not has_ranking:
            validation_issues.append("missing_ranking_structure")
        
        # Check 3: Contains specific evidence
        evidence_indicators = ['ì ', 'í”„ë¡œì íŠ¸', 'ì‚¬ë¡€', 'ê²½í—˜', 'ê¸°ìˆ ', 'ì—­ëŸ‰', 'ì ìˆ˜']
        mentioned_evidence = sum(1 for indicator in evidence_indicators if indicator in response_text)
        if mentioned_evidence < 3:
            validation_issues.append("insufficient_evidence_detail")
        
        # Check 4: Avoid generic/vague responses
        vague_phrases = ['ì „ë°˜ì ìœ¼ë¡œ', 'ì¼ë°˜ì ìœ¼ë¡œ', 'ëŒ€ì²´ë¡œ', 'í‰ê· ì ìœ¼ë¡œ', 'ë³´í†µ', 'ê·¸ëŸ°ëŒ€ë¡œ']
        vague_count = sum(1 for phrase in vague_phrases if phrase in response_text)
        if vague_count > 2:
            validation_issues.append("too_vague")
        
        return validation_issues
    
    # Validate initial response
    validation_issues = validate_hcx_response(hcx_recommendation, question)
    fallback_used = False
    final_recommendation = hcx_recommendation
    
    if validation_issues:
        print(f"âš ï¸ HCX validation issues detected: {validation_issues}")
        
        # Attempt fallback
        fallback_result = attempt_hcx_fallback(question, full_context, hcx_client)
        
        if fallback_result["success"]:
            fallback_issues = validate_hcx_response(fallback_result["recommendation"], question)
            
            if len(fallback_issues) < len(validation_issues):
                print(f"âœ… Fallback improved quality: {len(validation_issues)} â†’ {len(fallback_issues)} issues")
                final_recommendation = fallback_result["recommendation"]
                validation_issues = fallback_issues
                fallback_used = True
            else:
                print(f"âš ï¸ Fallback didn't improve significantly, using original response")
        else:
            print(f"âŒ Fallback failed: {fallback_result['error']}")
    else:
        print(f"âœ… HCX response passed validation checks")
    
    return {
        "final_recommendation": final_recommendation,
        "quality_metrics": {
            "validation_issues": validation_issues,
            "response_length": len(final_recommendation),
            "quality_score": max(0, 10 - len(validation_issues))
        },
        "fallback_used": fallback_used
    }


def attempt_hcx_fallback(question: str, full_context: str, hcx_client):
    """Attempt fallback with enhanced prompt for HCX"""
    fallback_prompt = f"""ì´ì „ ì‘ë‹µì´ ë„ˆë¬´ ê°„ëµí•˜ê±°ë‚˜ êµ¬ì²´ì„±ì´ ë¶€ì¡±í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ í‰ê°€ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë” ìƒì„¸í•˜ê³  êµ¬ì²´ì ì¸ MSP ì¶”ì²œì„ í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: "{question}"

{full_context}

=== ê°•í™”ëœ ë¶„ì„ ì§€ì¹¨ ===
1. ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ íšŒì‚¬ëª…ê³¼ ëª…í™•í•œ ìˆœìœ„ë¥¼ ì œì‹œí•  ê²ƒ
2. ê° ì¶”ì²œë§ˆë‹¤ í‰ê°€ ì ìˆ˜ì™€ êµ¬ì²´ì  í”„ë¡œì íŠ¸/ê¸°ìˆ  ê²½í—˜ì„ ì–¸ê¸‰í•  ê²ƒ
3. ì‚¬ìš©ì ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ ê´€ë ¨ì„±ì„ ëª…í™•íˆ í•  ê²ƒ
4. ì¼ë°˜ì /ì¶”ìƒì  í‘œí˜„ ëŒ€ì‹  êµ¬ì²´ì  ê·¼ê±°ì™€ ìˆ˜ì¹˜ë¥¼ í™œìš©í•  ê²ƒ

í•„ìˆ˜ ì‘ë‹µ í˜•ì‹:
**1ìˆœìœ„: [ì •í™•í•œ íšŒì‚¬ëª…]**
ì„ ì • ì´ìœ : [êµ¬ì²´ì  ê¸°ìˆ /ê²½í—˜ + í‰ê°€ì ìˆ˜ ì–¸ê¸‰]
ê´€ë ¨ ì¦ê±°: [êµ¬ì²´ì  Q&A ë‚´ìš©ê³¼ ì ìˆ˜]

**2ìˆœìœ„: [ì •í™•í•œ íšŒì‚¬ëª…]** 
ì„ ì • ì´ìœ : [1ìˆœìœ„ì™€ì˜ ì°¨ì´ì  + êµ¬ì²´ì  ê·¼ê±°]
ê´€ë ¨ ì¦ê±°: [êµ¬ì²´ì  Q&A ë‚´ìš©ê³¼ ì ìˆ˜]

ìµœì†Œ 400ì ì´ìƒ, êµ¬ì²´ì  ê·¼ê±° ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

    try:
        fallback_response = hcx_client.chat.completions.create(
            model="HCX-005",
            messages=[
                {"role": "system", "content": "MSP í‰ê°€ ì „ë¬¸ê°€ë¡œì„œ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë°ì´í„° ê¸°ë°˜ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤. ì¼ë°˜ì  í‘œí˜„ì„ í”¼í•˜ê³  êµ¬ì²´ì  ê·¼ê±°ì™€ ìˆ˜ì¹˜ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤."},
                {"role": "user", "content": fallback_prompt}
            ],
            top_p=0.5,
            temperature=0.2,
            max_tokens=1000
        )
        
        return {
            "success": True,
            "recommendation": fallback_response.choices[0].message.content.strip()
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }


def call_claude_critic(question: str, hcx_recommendation: str, full_context: str):
    """Call Claude to critically analyze HCX recommendation"""
    import anthropic
    import os
    
    critic_prompt = f"""ë‹¹ì‹ ì€ MSP í‰ê°€ì˜ í’ˆì§ˆ ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ë¥¸ AIê°€ ì œê³µí•œ ì¶”ì²œì„ ë¹„íŒì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì‚¬ìš©ì ì§ˆë¬¸: "{question}"

=== HCX ëª¨ë¸ì˜ ì¶”ì²œ ê²°ê³¼ ===
{hcx_recommendation}

=== ì›ë³¸ í‰ê°€ ë°ì´í„° ===
{full_context}

=== ë¹„íŒì  ë¶„ì„ ì§€ì¹¨ ===
1. **ê´€ë ¨ì„± ê²€ì¦**: ì¶”ì²œëœ íšŒì‚¬ë“¤ì´ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ê³¼ ì§ì ‘ ê´€ë ¨ì´ ìˆëŠ”ê°€?
2. **ê·¼ê±° í’ˆì§ˆ**: ì œì‹œëœ ê·¼ê±°ê°€ êµ¬ì²´ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ”ê°€?
3. **ëˆ„ë½ ë¶„ì„**: ë” ì í•©í•œ íšŒì‚¬ê°€ ëˆ„ë½ë˜ì—ˆëŠ”ê°€?
4. **ìˆœìœ„ íƒ€ë‹¹ì„±**: ì œì‹œëœ ìˆœìœ„ê°€ ë…¼ë¦¬ì ìœ¼ë¡œ íƒ€ë‹¹í•œê°€?
5. **í¸í–¥ ê²€ì¦**: íŠ¹ì • ìš”ì†Œ(ì ìˆ˜, íšŒì‚¬ í¬ê¸° ë“±)ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´í•˜ì§€ ì•Šì•˜ëŠ”ê°€?

=== ë¶„ì„ ê²°ê³¼ í˜•ì‹ ===
**ê²€ì¦ ê²°ê³¼**
- ì¶”ì²œ ì ì ˆì„±: [ì ì ˆ/ë¶€ì ì ˆ/ë¶€ë¶„ì ]
- ì£¼ìš” ê°•ì : [HCX ì¶”ì²œì˜ ì¢‹ì€ ì ë“¤]
- ë°œê²¬ëœ ë¬¸ì œ: [ë…¼ë¦¬ì  ì˜¤ë¥˜, ëˆ„ë½, í¸í–¥ ë“±]

**ê°œì„  ì œì•ˆ**
- ì¶”ê°€ ê³ ë ¤ì‚¬í•­: [ë†“ì¹œ ì¤‘ìš”í•œ ìš”ì†Œë“¤]
- ëŒ€ì•ˆ í›„ë³´: [ê³ ë ¤í•´ì•¼ í•  ë‹¤ë¥¸ íšŒì‚¬ë“¤]
- ìˆœìœ„ ì¡°ì • í•„ìš”ì„±: [ìˆœìœ„ ë³€ê²½ì´ í•„ìš”í•œ ì´ìœ ]"""

    claude_client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
    
    critic_response = claude_client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=1000,
        temperature=0.1,
        system="MSP í‰ê°€ í’ˆì§ˆ ê²€ì¦ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ë¥¸ AIì˜ ì¶”ì²œì„ ê°ê´€ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ê°œì„ ì ì„ ì œì‹œí•©ë‹ˆë‹¤.",
        messages=[{
            "role": "user", 
            "content": critic_prompt
        }]
    )
    
    critic_analysis = critic_response.content[0].text.strip()
    print(f"âœ… Critic Analysis length: {len(critic_analysis)} chars")
    
    return {
        "analysis": critic_analysis,
        "analysis_length": len(critic_analysis)
    }


def call_claude_refiner(question: str, hcx_recommendation: str, critic_analysis: str, full_context: str):
    """Call Claude to create final refined recommendation"""
    import anthropic
    import os
    
    refiner_prompt = f"""ë‹¹ì‹ ì€ MSP ì„ ì • ìµœì¢… ì˜ì‚¬ê²°ì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ˆê¸° ì¶”ì²œê³¼ ë¹„íŒì  ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ìµœì¢… ì¶”ì²œì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{question}"

=== ì´ˆê¸° ì¶”ì²œ (HCX) ===
{hcx_recommendation}

=== ë¹„íŒì  ë¶„ì„ (Claude Critic) ===
{critic_analysis}

=== ì›ë³¸ í‰ê°€ ë°ì´í„° ===
{full_context}

=== ìµœì¢… ì¶”ì²œ ì§€ì¹¨ ===
1. ì´ˆê¸° ì¶”ì²œì˜ ì¥ì ì„ ìœ ì§€í•˜ë©´ì„œ ë¹„íŒì  ë¶„ì„ì˜ ê°œì„ ì  ë°˜ì˜
2. ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ê³¼ì˜ ì§ì ‘ì  ë¶€í•©ë„ë¥¼ ìµœìš°ì„  ê³ ë ¤
3. ëª¨ë“  ê²°ë¡ ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ê²€ì¦ ê°€ëŠ¥í•œ ê·¼ê±° ì œì‹œ
4. ì„ íƒí•˜ì§€ ì•Šì€ íšŒì‚¬ë“¤ì— ëŒ€í•œ ëª…í™•í•œ ì œì™¸ ì´ìœ  ì„¤ëª…

=== ìµœì¢… ì‘ë‹µ í˜•ì‹ ===
**[ìš”êµ¬ì‚¬í•­ ë¶„ì„]**
í•µì‹¬ ìš”êµ¬ì‚¬í•­: [ì‚¬ìš©ìê°€ ì°¾ëŠ” ê²ƒ]
ì¤‘ìš” í‚¤ì›Œë“œ: [ê¸°ìˆ /ì¡°ê±´ í‚¤ì›Œë“œë“¤]

**[ìµœì¢… ì¶”ì²œ]**
**1ìˆœìœ„: [íšŒì‚¬ëª…]**
**ì„ ì • ì´ìœ :** [í•µì‹¬ ê·¼ê±° 1-2ë¬¸ì¥]
**ê´€ë ¨ ì¦ê±°:**
â€¢ [ì§ì ‘ ê´€ë ¨ Q&A 1]: [ìš”ì•½] (ì ìˆ˜: Xì )
â€¢ [ì§ì ‘ ê´€ë ¨ Q&A 2]: [ìš”ì•½] (ì ìˆ˜: Xì )

**2ìˆœìœ„: [íšŒì‚¬ëª…]**
**ì„ ì • ì´ìœ :** [1ìˆœìœ„ ëŒ€ë¹„ ì°¨ì´ì ê³¼ ì¥ì ]
**ê´€ë ¨ ì¦ê±°:**
â€¢ [ì§ì ‘ ê´€ë ¨ Q&A]: [ìš”ì•½] (ì ìˆ˜: Xì )

**[ìµœì¢… íŒë‹¨ ê·¼ê±°]**
- **ìˆœìœ„ ê²°ì • ì´ìœ :** [êµ¬ì²´ì  ì°¨ì´ì ]
- **ì œì™¸ëœ íšŒì‚¬ë“¤:** [ë‹¤ë¥¸ íšŒì‚¬ ì œì™¸ ì´ìœ ]
- **ì¶”ì²œ ì‹ ë¢°ë„:** [ìƒ/ì¤‘/í•˜] - [ì‹ ë¢°ë„ ê·¼ê±°]"""

    claude_client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
    
    final_response = claude_client.messages.create(
        model="claude-3-haiku-20240307",
        max_tokens=1500,
        temperature=0.1,
        system="MSP ì„ ì • ìµœì¢… ì˜ì‚¬ê²°ì • ì „ë¬¸ê°€ë¡œì„œ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì ì˜ ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤.",
        messages=[{
            "role": "user", 
            "content": refiner_prompt
        }]
    )
    
    final_recommendation = final_response.content[0].text.strip()
    print(f"âœ… Final Recommendation length: {len(final_recommendation)} chars")
    
    # Post-processing
    final_recommendation = final_recommendation.replace("ì„¤ë£¨ì…˜", "ì†”ë£¨ì…˜")
    
    return {
        "recommendation": final_recommendation,
        "recommendation_length": len(final_recommendation)
    }


def compile_final_response(final_recommendation: str, raw_data: dict, context_data: dict, 
                          hcx_result: dict, critic_result: dict, final_result: dict):
    """Compile all results into final response format"""
    
    return {
        "answer": final_recommendation,
        "evidence": raw_data["query_results"]["metadatas"][0],
        "model_used": "multi-llm-hcx-claude-critic-refiner",
        "analysis_quality": "multi_model_validated",
        "companies_analyzed": len(raw_data["grouped_chunks"]),
        "context_management": context_data["context_stats"],
        "hcx_quality_metrics": hcx_result["quality_metrics"],
        "processing_steps": {
            "step1_hcx_responder": hcx_result["quality_metrics"]["response_length"],
            "step2_claude_critic": critic_result["analysis_length"], 
            "step3_claude_refiner": final_result["recommendation_length"]
        },
        "intermediate_outputs": {
            "hcx_initial": hcx_result["recommendation"],
            "claude_criticism": critic_result["analysis"]
        },
        "system_metadata": {
            "fallback_used": hcx_result["fallback_used"],
            "validation_passed": len(hcx_result["quality_metrics"]["validation_issues"]) == 0,
            "context_selection_method": "avg_score_based"
        }
    }
