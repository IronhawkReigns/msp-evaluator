from fpdf import FPDF
from datetime import datetime
from fastapi import HTTPException
from typing import Any
import requests
import json
import uuid
from difflib import get_close_matches
from vector_writer import clova_embedding
import os
import chromadb
from chromadb import PersistentClient

# Embedding and collection setup
def query_embed(text: str):
    return clova_embedding(text)

CHROMA_PATH = os.path.abspath("chroma_store")
client = PersistentClient(path=CHROMA_PATH)
collection = client.get_or_create_collection("msp_chunks")

import anthropic
import os
from collections import defaultdict
import traceback
from fastapi import HTTPException

def run_msp_recommendation(question: str, min_score: int):
    """
    Expert-quality MSP recommendation with simple format but professional depth
    """
    try:
        query_vector = query_embed(question)
        query_results = collection.query(
            query_embeddings=[query_vector],
            n_results=15
        )
        grouped_chunks = defaultdict(list)
        for meta in query_results["metadatas"][0]:
            if not isinstance(meta.get("answer"), str) or not meta["answer"].strip():
                continue
            if meta["score"] is not None and int(meta["score"]) >= min_score:
                grouped_chunks[meta["msp_name"]].append({
                    "question": meta['question'],
                    "answer": meta['answer'],
                    "score": meta['score'],
                    "category": meta.get('category', 'ë¯¸ë¶„ë¥˜')
                })

        if not grouped_chunks:
            return {"answer": "í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” í‰ê°€ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        # Enhanced analysis for expert-level insights
        context_blocks = []
        company_insights = {}
        
        for msp, qa_list in grouped_chunks.items():
            scores = [qa['score'] for qa in qa_list]
            avg_score = sum(scores) / len(scores)
            high_scores = [qa for qa in qa_list if qa['score'] >= 4]
            
            # Analyze answer quality and specificity
            detailed_answers = [qa for qa in qa_list if len(qa['answer']) > 100]
            specific_evidence = [qa for qa in qa_list if any(keyword in qa['answer'].lower() 
                               for keyword in ['í”„ë¡œì íŠ¸', 'ê²½í—˜', 'ì‚¬ë¡€', 'ë…„', 'ê°œì›”', '%', 'ëª…', 'ê±´'])]
            
            company_insights[msp] = {
                'avg_score': avg_score,
                'excellence_count': len(high_scores),
                'detail_quality': len(detailed_answers),
                'evidence_strength': len(specific_evidence),
                'total_responses': len(qa_list)
            }
            
            # Create rich context with best evidence
            qa_details = []
            # Sort by score and answer specificity
            sorted_qa = sorted(qa_list, key=lambda x: (x['score'], len(x['answer'])), reverse=True)
            for qa in sorted_qa[:4]:
                qa_details.append(f"Q: {qa['question']}\nA: {qa['answer']}\nì ìˆ˜: {qa['score']}/5")
            
            context_blocks.append(f"[{msp}]\n" + "\n\n".join(qa_details))

        context = "\n\n".join(context_blocks)
        
        # Expert-level prompt for professional reasoning
        prompt = f"""ë‹¹ì‹ ì€ 10ë…„ ì´ìƒì˜ í´ë¼ìš°ë“œ ì»¨ì„¤íŒ… ê²½í—˜ì„ ê°€ì§„ MSP ì„ ì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ MSP íŒŒíŠ¸ë„ˆì‚¬ í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ íšŒì‚¬ë“¤ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”. ëª‡ ê°œ íšŒì‚¬ë¥¼ ì¶”ì²œí• ì§€ëŠ” ìƒí™©ì— ë”°ë¼ ê²°ì •í•´ ì£¼ì„¸ìš”.:

{context}

ì‚¬ìš©ì ì§ˆë¬¸: "{question}"

ë‹¤ìŒ ì „ë¬¸ê°€ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•˜ì‹­ì‹œì˜¤:
â€¢ ë‹µë³€ì˜ êµ¬ì²´ì„±ê³¼ ì‹¤ë¬´ ê²½í—˜ì˜ ê¹Šì´
â€¢ ê´€ë ¨ ê¸°ìˆ  ì—­ëŸ‰ì˜ ì‹¤ì œ ì…ì¦ ì •ë„  
â€¢ ìœ ì‚¬ í”„ë¡œì íŠ¸ ìˆ˜í–‰ ê²½í—˜ê³¼ ì„±ê³¼
â€¢ ê¸°ìˆ ì  ì°¨ë³„í™” ìš”ì†Œì™€ ì „ë¬¸ì„±
â€¢ ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ ì°½ì¶œ ê°€ëŠ¥ì„±

ì‘ë‹µ í˜•ì‹:
**1ìœ„ ì¶”ì²œ: [íšŒì‚¬ëª…]**
- ì¶”ì²œ ì´ìœ : [ì „ë¬¸ê°€ ê´€ì ì˜ í•µì‹¬ ê·¼ê±° 2-3ë¬¸ì¥ - ë°˜ë“œì‹œ êµ¬ì²´ì  ê²½í—˜ì´ë‚˜ ì—­ëŸ‰ì„ ì–¸ê¸‰]
- í•µì‹¬ ê°•ì : [í•´ë‹¹ ì˜ì—­ì—ì„œì˜ ê²€ì¦ëœ ì „ë¬¸ì„±]
- ê´€ë ¨ ì ìˆ˜: [ê´€ë ¨ í‰ê°€ ì ìˆ˜ë“¤]

**2ìœ„ ì¶”ì²œ: [íšŒì‚¬ëª…]**  
- ì¶”ì²œ ì´ìœ : [ì°¨ë³„í™”ëœ ê°•ì ê³¼ ê·¼ê±° 2-3ë¬¸ì¥]
- í•µì‹¬ ê°•ì : [1ìœ„ì™€ êµ¬ë³„ë˜ëŠ” ì „ë¬¸ì„±]
- ê´€ë ¨ ì ìˆ˜: [ê´€ë ¨ í‰ê°€ ì ìˆ˜ë“¤]

ì¤‘ìš”: ë°˜ë“œì‹œ í‰ê°€ ë°ì´í„°ì— ëª…ì‹œëœ êµ¬ì²´ì  ì‚¬ë¡€, ê²½í—˜, ìˆ˜ì¹˜ë¥¼ ê·¼ê±°ë¡œ ì œì‹œí•˜ê³ , ì¶”ìƒì ì´ê±°ë‚˜ ì¼ë°˜ì ì¸ í‘œí˜„ì€ í”¼í•˜ì‹­ì‹œì˜¤. ì‹¤ì œ ì „ë¬¸ê°€ê°€ ê²€í† í•´ë„ ë…¼ë¦¬ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ì¶”ì²œì´ ë˜ë„ë¡ ì‘ì„±í•˜ì‹­ì‹œì˜¤."""

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Vector search failed: {str(e)}")

    # In your run_msp_recommendation function, around line 100-130
# Replace this section:

    try:
        client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        
        response = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=1000,
            temperature=0.15,  # Low temperature for consistent, professional reasoning
            system="ë‹¹ì‹ ì€ í´ë¼ìš°ë“œ ë° MSP ì„ ì • ë¶„ì•¼ì˜ ì‹œë‹ˆì–´ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. í•­ìƒ ë°ì´í„°ì— ê¸°ë°˜í•œ ë…¼ë¦¬ì ì´ê³  êµ¬ì²´ì ì¸ ì¶”ì²œì„ ì œê³µí•˜ë©°, ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ í†µì°°ë ¥ì„ ë³´ì—¬ì£¼ì‹­ì‹œì˜¤. ì¶”ìƒì  í‘œí˜„ë³´ë‹¤ëŠ” êµ¬ì²´ì  ê·¼ê±°ì™€ ì‹¤ë¬´ì  ê´€ì ì„ ì¤‘ì‹œí•©ë‹ˆë‹¤.",
            messages=[{
                "role": "user", 
                "content": prompt
            }]
        )
        
        answer = response.content[0].text.strip()
        
        # Quality enhancements for professional consistency
        answer = answer.replace("ì„¤ë£¨ì…˜", "ì†”ë£¨ì…˜")
        answer = answer.replace("ìˆìŠµë‹ˆë‹¤", "ìˆìŒ")  # More concise professional tone
        answer = answer.replace("í•©ë‹ˆë‹¤", "í•¨")
        
        # Ensure professional terminology consistency
        professional_terms = {
            "êµ¬í˜„": "êµ¬ì¶•", 
            "ë§Œë“¤": "êµ¬ì¶•",
            "í•´ê²°": "í•´ê²°",
            "ì œê³µ": "ì œê³µ"
        }
        
        for old_term, new_term in professional_terms.items():
            answer = answer.replace(old_term, new_term)
        
        return {
            "answer": answer,
            "evidence": query_results["metadatas"][0],
            "model_used": "claude-3-haiku-expert",
            "analysis_quality": "expert_validated"
        }
        
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Claude API error: {str(e)}")

def run_msp_recommendation_clova(question: str, min_score: int):
    """
    Original CLOVA-based MSP recommendation function (backup)
    """
    from collections import defaultdict
    import traceback
    from openai import OpenAI
    import json

    try:
        query_vector = query_embed(question)
        query_results = collection.query(
            query_embeddings=[query_vector],
            n_results=10
        )
        grouped_chunks = defaultdict(list)
        for meta in query_results["metadatas"][0]:
            if not isinstance(meta.get("answer"), str) or not meta["answer"].strip():
                continue
            if meta["score"] is not None and int(meta["score"]) >= min_score:
                grouped_chunks[meta["msp_name"]].append(
                    f"Q: {meta['question']}\nA: {meta['answer']} (score: {meta['score']})"
                )

        if not grouped_chunks:
            return {"answer": "í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” í‰ê°€ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        context_blocks = []
        for msp, qa_list in grouped_chunks.items():
            context_blocks.append(f"[{msp}]\n" + "\n".join(qa_list))

        context = "\n\n".join(context_blocks)
        prompt = (
            f"{context}\n\n"
            f"ìœ„ì˜ Q&A ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ '{question}'ì— ê°€ì¥ ì˜ ë¶€í•©í•˜ëŠ” ìƒìœ„ 2ê°œ íšŒì‚¬ë¥¼ ì„ ì •í•´ ì£¼ì„¸ìš”.\n\n"
            f"[ì£¼ì˜ì‚¬í•­]\n"
            f"- ì¶”ë¡  ê¸ˆì§€: ì£¼ì–´ì§„ ì •ë³´ì— ëª…í™•íˆ ë‚˜íƒ€ë‚˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì •í•˜ê±°ë‚˜ ì¼ë°˜ì ì¸ ê¸°ëŒ€ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŒë‹¨í•˜ì§€ ë§ˆì„¸ìš”.\n"
            f"- ì •ë³´ ë¶€ì¡± ì‹œ í•´ë‹¹ íšŒì‚¬ë¥¼ ì œì™¸í•˜ê³ , ëª…í™•í•œ ì—°ê²°ê³ ë¦¬ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì„ ì •í•˜ì„¸ìš”.\n"
            f"- scoreëŠ” ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ë³´ì¡° ì§€í‘œì¼ ë¿ì´ë©°, ë°˜ë“œì‹œ ë†’ì€ ì ìˆ˜ê°€ ì§ì ‘ì ì¸ ë‹µë³€ì„ ì˜ë¯¸í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.\n"
            f"- ë§ì¶¤ë²•ê³¼ ë¬¸ë²•ì— ìœ ì˜í•˜ì—¬ ì˜¤íƒ€ ì—†ì´ ì‘ì„±í•  ê²ƒ\n\n"
            f"[í‰ê°€ ê¸°ì¤€]\n"
            f"1. ì§ˆë¬¸ì— ëª…ì‹œì ìœ¼ë¡œ ë‹µí•˜ê³  ìˆëŠ”ê°€?\n"
            f"2. ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?\n"
            f"3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì‚¬ë¡€, ê·¼ê±°ê°€ ìˆëŠ”ê°€?\n"
            f"4. ì ìˆ˜ëŠ” ë³´ì¡°ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ê³ , ì‘ë‹µ ë‚´ìš©ì˜ ëª…í™•ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€í•  ê²ƒ\n"
            f"   ì˜ˆ: 'UI/UX' ê´€ë ¨ ì§ˆë¬¸ì˜ ê²½ìš° 'ì‚¬ìš© í¸ì˜ì„±', 'ì¸í„°í˜ì´ìŠ¤', 'ì ‘ê·¼ì„±', 'ì§ê´€ì„±' ë“± í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸\n\n"
            f"[ì œì™¸ ê¸°ì¤€]\n"
            f"- ë³´ì•ˆ, ì„±ëŠ¥, ë°ì´í„° ì²˜ë¦¬ ë“± ìœ ì‚¬ ê°œë…ì€ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µí•˜ì§€ ì•ŠëŠ” í•œ ì œì™¸\n"
            f"- ì¶”ì¸¡, ê¸°ëŒ€ ê¸°ë°˜ í•´ì„, ì ìˆ˜ë§Œì„ ê·¼ê±°ë¡œ í•œ ì„ ì •ì€ ê¸ˆì§€\n"
            f"- DBì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°ì—…ì„ ì„ ì •í•˜ëŠ” ê²ƒì€ ì ˆëŒ€ ê¸ˆì§€\n\n"
            f"[ì‘ë‹µ í˜•ì‹]\n"
            f"- ê° íšŒì‚¬ëª…ì„ **êµµê²Œ** í‘œì‹œí•˜ê³ , ê° íšŒì‚¬ë¥¼ ë³„ë„ì˜ ë‹¨ë½ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”.\n"
            f"- ìµœì¢… ì‘ë‹µ ì „ íšŒì‚¬ëª…ì´ msp_nameì´ ë§ëŠ”ì§€ í™•ì‹¤íˆ í™•ì¸ í›„ ì‘ë‹µí•´ ì£¼ì„¸ìš”.\n"
            f"- ì„ ì • ì´ìœ ëŠ” ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ 1~2ë¬¸ì¥ìœ¼ë¡œ ê¸°ìˆ í•˜ì„¸ìš”.\n\n"
            f"ì˜ˆì‹œ:\n"
            f"**A íšŒì‚¬**\n"
            f"- ì„ ì • ì´ìœ : AI ì „ë¬¸ ì¸ë ¥ ë¹„ìœ¨ì´ ë†’ê³ , í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ í”„ë¡œì íŠ¸ ì‚¬ë¡€ë¥¼ ì–¸ê¸‰í•˜ë©° 5ì ì„ ë°›ìŒ\n\n"
            f"**B íšŒì‚¬**\n"
            f"- ì„ ì • ì´ìœ : OCR ê¸°ìˆ  ê´€ë ¨ ê²½í—˜ì„ ë³´ìœ í•˜ê³  ìˆìœ¼ë©°, í•´ë‹¹ ì§ˆë¬¸ì— ëª…í™•íˆ ì‘ë‹µí•˜ê³  4ì ì„ ê¸°ë¡í•¨\n\n"
            f"**ê¸°íƒ€ íšŒì‚¬**\n"
            f"- ê´€ë ¨ í‚¤ì›Œë“œ ë¶€ì¬, ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì  ë‹µë³€ ì—†ìŒ ë“± ëª…í™•í•œ ê·¼ê±°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ê°„ë‹¨íˆ ì–¸ê¸‰"
        )
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Vector search failed: {str(e)}")

    CLOVA_API_KEY = os.getenv("CLOVA_API_KEY_OPENAI")
    API_URL = "https://clovastudio.stream.ntruss.com/v1/openai"
    client = OpenAI(api_key=CLOVA_API_KEY, base_url=API_URL)
    model = "HCX-005"

    try:
        clova_response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "í´ë¼ìš°ë“œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ë¬¸ì¥ìœ¼ë¡œ, ì˜¤íƒˆì ì—†ì´ ì •í™•í•œ ë§ì¶¤ë²•ê³¼ ë¬¸ë²•ì„ ì‚¬ìš©í•´ ì£¼ì„¸ìš”. ë¬¸ì¥ì€ ê°„ê²°í•˜ë©´ì„œë„ ìì—°ìŠ¤ëŸ½ê³ , ì¼ê´€ë˜ë©° ì‹ ë¢°ê° ìˆê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            top_p=0.6,
            temperature=0.3,
            max_tokens=500
        )
        if not clova_response.choices or not clova_response.choices[0].message.content:
            answer = ""
        else:
            answer = clova_response.choices[0].message.content.strip()
        answer = answer.replace("ì„¤ë£¨ì…˜", "ì†”ë£¨ì…˜")
        return {"answer": answer, "raw": clova_response.model_dump(), "evidence": query_results["metadatas"][0]}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"HyperCLOVA error: {str(e)}")
    
# Information summary function (for Information domain)
def run_msp_information_summary(question: str):
    import traceback
    from openai import OpenAI
    import json

    query = question
    msp_name = extract_msp_name(question)

    all_results = collection.get(include=["metadatas"])
    all_msp_names = [meta.get("msp_name", "") for meta in all_results["metadatas"] if meta.get("msp_name")]

    matches = get_close_matches(msp_name, all_msp_names, n=1, cutoff=0.6)
    if not matches:
        return {"answer": "ì§ˆë¬¸í•˜ì‹  íšŒì‚¬ëª…ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.", "advanced": False}
    best_match = matches[0]

    try:
        query_vector = query_embed(question)
        query_results = collection.query(
            query_embeddings=[query_vector],
            n_results=8
        )
        filtered_chunks = [c for c in query_results["metadatas"][0] if c.get("answer") and c.get("question") and c.get("msp_name") == best_match]
        if not filtered_chunks:
            return {"answer": "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "advanced": False}

        answer_blocks = []
        for chunk in filtered_chunks:
            if not chunk.get("answer") or not chunk.get("question"):
                continue
            answer_blocks.append(f"Q: {chunk['question']}\nA: {chunk['answer']}")

        context = "\n\n".join(answer_blocks)
        prompt = (
            f"ë‹¤ìŒì€ MSP íŒŒíŠ¸ë„ˆì‚¬ ê´€ë ¨ ì¸í„°ë·° Q&A ëª¨ìŒì…ë‹ˆë‹¤. ì•„ë˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì‘ë‹µí•´ ì£¼ì„¸ìš”.\n"
            f"ì‚¬ìš©ì ì§ˆë¬¸: \"{question}\"\n\n"
            f"{context}\n\n"
            f"[ì‘ë‹µ ì§€ì¹¨]\n"
            f"- ì‹¤ì œ Q&Aì— ê¸°ë°˜í•´ ìš”ì•½í•˜ê±°ë‚˜ ì¢…í•©ì ìœ¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\n"
            f"- ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ë¡ í•˜ê±°ë‚˜ ê¾¸ë©°ë‚´ì§€ ë§ˆì„¸ìš”.\n"
            f"- ì§ˆë¬¸ê³¼ ë‹¤ë¥¸ íƒ€ íšŒì‚¬ì˜ ì •ë³´ë¥¼ ì ˆëŒ€ë¡œ ì–µì§€ë¡œ ë¼ì›Œë§ì¶”ì§€ ë§ˆì„¸ìš”."
            f"- ê°€ëŠ¥í•œ í•œ ê°„ê²°í•˜ë©´ì„œë„ ì‹ ë¢°ë„ ìˆëŠ” í‘œí˜„ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n"
        )
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Vector search failed: {str(e)}")

    CLOVA_API_KEY = os.getenv("CLOVA_API_KEY_OPENAI")
    API_URL = "https://clovastudio.stream.ntruss.com/v1/openai"
    client = OpenAI(api_key=CLOVA_API_KEY, base_url=API_URL)
    model = "HCX-005"

    try:
        clova_response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ì •í™•í•œ ì •ë³´ì— ê¸°ë°˜í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ í•´ì£¼ì„¸ìš”. ì˜¤íƒˆì ì—†ì´ ëª…í™•í•˜ê³  ì¼ê´€ëœ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            top_p=0.6,
            temperature=0.3,
            max_tokens=500
        )
        if not clova_response.choices or not clova_response.choices[0].message.content:
            answer = ""
        else:
            answer = clova_response.choices[0].message.content.strip()
        answer = answer.replace("ì„¤ë£¨ì…˜", "ì†”ë£¨ì…˜")
        return {"answer": answer, "raw": clova_response.model_dump(), "advanced": False, "evidence": filtered_chunks}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"HyperCLOVA error: {str(e)}")
    
def run_msp_information_summary_claude(question: str):
    """Direct Claude version without Perplexity"""
    import traceback
    
    query = question
    msp_name = extract_msp_name(question)

    all_results = collection.get(include=["metadatas"])
    all_msp_names = [meta.get("msp_name", "") for meta in all_results["metadatas"] if meta.get("msp_name")]

    matches = get_close_matches(msp_name, all_msp_names, n=1, cutoff=0.6)
    if not matches:
        return {"answer": "ì§ˆë¬¸í•˜ì‹  íšŒì‚¬ëª…ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.", "advanced": False}
    best_match = matches[0]

    try:
        query_vector = query_embed(question)
        query_results = collection.query(
            query_embeddings=[query_vector],
            n_results=8
        )
        filtered_chunks = [c for c in query_results["metadatas"][0] if c.get("answer") and c.get("question") and c.get("msp_name") == best_match]
        if not filtered_chunks:
            return {"answer": "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "advanced": False}

        answer_blocks = []
        for chunk in filtered_chunks:
            if not chunk.get("answer") or not chunk.get("question"):
                continue
            answer_blocks.append(f"Q: {chunk['question']}\nA: {chunk['answer']}")

        context = "\n\n".join(answer_blocks)
        prompt = f"""ë‹¤ìŒì€ {best_match}ì— ëŒ€í•œ ì¸í„°ë·° Q&Aì…ë‹ˆë‹¤:

{context}

ì‚¬ìš©ì ì§ˆë¬¸: "{question}"

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì£¼ì–´ì§„ ì •ë³´ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ë¡ í•˜ì§€ ë§ˆì„¸ìš”."""

        client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        
        response = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=500,
            temperature=0.3,
            messages=[{"role": "user", "content": prompt}]
        )
        
        answer = response.content[0].text.strip()
        answer = answer.replace("ì„¤ë£¨ì…˜", "ì†”ë£¨ì…˜")
        
        return {"answer": answer, "advanced": False, "evidence": filtered_chunks}
        
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Claude API error: {str(e)}")

def run_msp_information_summary_pplx(question: str):
    import traceback
    import requests
    import os

    query = question
    msp_name = extract_msp_name(question)

    all_results = collection.get(include=["metadatas"])
    all_msp_names = [meta.get("msp_name", "") for meta in all_results["metadatas"] if meta.get("msp_name")]

    from difflib import get_close_matches
    matches = get_close_matches(msp_name, all_msp_names, n=1, cutoff=0.6)
    if not matches:
        return {"answer": "ì§ˆë¬¸í•˜ì‹  íšŒì‚¬ëª…ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.", "advanced": True}
    best_match = matches[0]

    try:
        query_vector = query_embed(question)
        query_results = collection.query(
            query_embeddings=[query_vector],
            n_results=8
        )
        filtered_chunks = [c for c in query_results["metadatas"][0] if c.get("answer") and c.get("question") and c.get("msp_name") == best_match]
        if not filtered_chunks:
            return {"answer": "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "advanced": True}

        answer_blocks = []
        for chunk in filtered_chunks:
            if not chunk.get("answer") or not chunk.get("question"):
                continue
            answer_blocks.append(f"Q: {chunk['question']}\nA: {chunk['answer']}")

        context = "\n\n".join(answer_blocks)
        prompt = (
            f"{context}\n\n"
            f"ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n"
            f"\"{question}\"\n\n"
            f"[ì‘ë‹µ ê°€ì´ë“œë¼ì¸]\n"
            f"- ì•„ë˜ Q&AëŠ” ì°¸ê³ ìš©ì¼ ë¿ì´ë©°, ë” ì •í™•í•˜ê±°ë‚˜ í’ë¶€í•œ ì •ë³´ê°€ ìˆë‹¤ë©´ ì›¹ ê¸°ë°˜ì˜ ì§€ì‹ë„ ììœ ë¡­ê²Œ í™œìš©í•´ ì£¼ì„¸ìš”.\n"
            f"- ê·¼ê±°ê°€ ëª…í™•í•œ ê²½ìš°, ì£¼ì–´ì§„ ì •ë³´ ì™¸ì˜ ë°°ê²½ì§€ì‹ë„ ì ê·¹ í™œìš©í•´ ì£¼ì„¸ìš”.\n"
            f"- ë¬¸ì¥ì€ ìì—°ìŠ¤ëŸ½ê³  ì‹ ë¢°ê° ìˆê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n"
            f"- ì§€ë‚˜ì¹˜ê²Œ í˜•ì‹ì„ ê°•ì¡°í•˜ê¸°ë³´ë‹¤ëŠ”, ëª…í™•í•˜ê³  ìœ ìµí•œ ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í•´ ì£¼ì„¸ìš”.\n"
            f"- íšŒì‚¬ëª…ì€ ëª…í™•íˆ ì–¸ê¸‰í•˜ë˜, ë°˜ë³µì„ í”¼í•˜ê³  ë¬¸ë§¥ì— ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ ì£¼ì„¸ìš”."
        )
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Vector search failed: {str(e)}")

    try:
        response = requests.post(
            "https://api.perplexity.ai/chat/completions",
            headers={
                "Authorization": f"Bearer {os.getenv('PPLX_API_KEY')}",
                "Content-Type": "application/json",
            },
            json={
                "model": "sonar",
                "messages": [
                    {"role": "system", "content": "ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ë¥¼ ê°„ê²°í•œ í•œêµ­ì–´ë¡œ ì œê³µí•˜ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ]
            },
            timeout=30
        )
        print(f"ğŸ” Claude API status: {response.status_code}")
        print(f"ğŸ“¦ Claude API raw response: {response.text}")
        if response.status_code == 200:
            import re
            result = response.json()
            answer = result["choices"][0]["message"]["content"].strip()
            # Clean up answer
            answer = re.sub(r"\[Q&A\]", "", answer)
            answer = re.sub(r"Q[:ï¼š]", "", answer)
            answer = re.sub(r"A[:ï¼š]", "", answer)
            answer = answer.strip()
            answer = re.sub(r"\[\d+\]", "", answer)  # Remove [1], [2], etc.
            return {"answer": answer, "advanced": True, "evidence": filtered_chunks}
        else:
            return {"answer": "Claude API í˜¸ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.", "advanced": True}
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Claude API error: {str(e)}")

def extract_msp_name(question: str) -> str:
    from openai import OpenAI
    import os

    CLOVA_API_KEY = os.getenv("CLOVA_API_KEY_OPENAI")
    API_URL = "https://clovastudio.stream.ntruss.com/v1/openai"
    client = OpenAI(api_key=CLOVA_API_KEY, base_url=API_URL)
    model = "HCX-005"

    prompt = (
        f"ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ì‹¤ì œ í´ë¼ìš°ë“œ MSP íŒŒíŠ¸ë„ˆì‚¬ì˜ ì´ë¦„ë§Œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì„¸ìš”. ë¬¸ì¥ ì „ì²´ë¥¼ ì¶œë ¥í•˜ì§€ ë§ê³ , íšŒì‚¬ëª…ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n"
        f"[ì˜ˆì‹œ]\n"
        f"ì§ˆë¬¸: 'ITCEN CLOITì— ëŒ€í•´ ì•Œë ¤ì¤˜'\nì‘ë‹µ: ITCEN CLOIT\n"
        f"ì§ˆë¬¸: 'Lominì˜ AI ì—­ëŸ‰ì€?'\nì‘ë‹µ: Lomin\n"
        f"ì§ˆë¬¸: 'ë² ìŠ¤í•€ê¸€ë¡œë²Œì˜ MLOps ì‚¬ë¡€ëŠ”?'\nì‘ë‹µ: ë² ìŠ¤í•€ê¸€ë¡œë²Œ\n"
        f"ì§ˆë¬¸: '{question}'\n"
        f"ì‘ë‹µ:"
    )

    try:
        clova_response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ì§ˆë¬¸ì—ì„œ í´ë¼ìš°ë“œ MSP íšŒì‚¬ ì´ë¦„ë§Œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•´ ì£¼ì„¸ìš”. ë¬¸ì¥ì€ ì ˆëŒ€ ì‘ì„±í•˜ì§€ ë§ê³ , íšŒì‚¬ëª…ë§Œ ë‹¨ë…ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”. ì˜ˆ: ë² ìŠ¤í•€ê¸€ë¡œë²Œ"},
                {"role": "user", "content": prompt}
            ],
            top_p=0.6,
            temperature=0.3,
            max_tokens=20
        )
        raw = clova_response.choices[0].message.content.strip()
        print(f"ğŸ” Extracted raw MSP name: {raw}")
        return raw
    except Exception as e:
        print(f"âŒ Error extracting MSP name: {e}")
        return ""

def run_msp_news_summary_clova(question: str):
    import urllib.parse
    import urllib.request
    import traceback

    msp_name = extract_msp_name(question)
    if not msp_name:
        return {"answer": "íšŒì‚¬ëª…ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.", "advanced": True}

    # Get vector DB information for the MSP
    try:
        query_vector = query_embed(question)
        query_results = collection.query(
            query_embeddings=[query_vector],
            n_results=10
        )
        db_chunks = [
            f"Q: {chunk['question']}\nA: {chunk['answer']}"
            for chunk in query_results["metadatas"][0]
            if chunk.get("msp_name") == msp_name and chunk.get("question") and chunk.get("answer")
        ][:5]
        db_context = "\n\n".join(db_chunks)
    except Exception as e:
        db_context = ""

    try:
        query = urllib.parse.quote(msp_name)
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=10&sort=sim"
        headers = {
            "X-Naver-Client-Id": os.getenv("NAVER_CLIENT_ID"),
            "X-Naver-Client-Secret": os.getenv("NAVER_CLIENT_SECRET")
        }
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req) as response:
            if response.status != 200:
                raise Exception(f"Naver API Error: {response.status}")
            news_data = json.loads(response.read().decode("utf-8"))

        url_web = f"https://openapi.naver.com/v1/search/webkr.json?query={query}&display=3&sort=sim"
        req_web = urllib.request.Request(url_web, headers=headers)
        with urllib.request.urlopen(req_web) as response_web:
            if response_web.status != 200:
                raise Exception(f"Naver Web API Error: {response_web.status}")
            web_data = json.loads(response_web.read().decode("utf-8"))

        if "items" not in news_data or not news_data["items"]:
            return {"answer": f"{msp_name}ì— ëŒ€í•œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "advanced": True}

        article_summaries = "\n".join(
            f"- ì œëª©: {item['title'].replace('<b>', '').replace('</b>', '')}\n  ìš”ì•½: {item['description'].replace('<b>', '').replace('</b>', '')}"
            for item in news_data["items"]
        )

        web_summaries = "\n".join(
            f"- ì œëª©: {item['title'].replace('<b>', '').replace('</b>', '')}\n  ìš”ì•½: {item['description'].replace('<b>', '').replace('</b>', '')}"
            for item in web_data.get("items", [])
        )

        prompt = (
            f"ë‹¤ìŒì€ í´ë¼ìš°ë“œ MSP ê¸°ì—… '{msp_name}'ì— ëŒ€í•œ ë‰´ìŠ¤ ê¸°ì‚¬, ì›¹ ë¬¸ì„œ, ì¸í„°ë·° Q&A ìš”ì•½ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì‘ë‹µí•´ ì£¼ì„¸ìš”.\n"
            f"ì‚¬ìš©ì ì§ˆë¬¸: \"{question}\"\n\n"
            f"[DB ê¸°ë°˜ ì •ë³´]\n{db_context}\n\n"
            f"[ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½]\n{article_summaries}\n\n"
            f"[ì›¹ ë¬¸ì„œ ìš”ì•½]\n{web_summaries}\n\n"
            f"[ì‘ë‹µ ì§€ì¹¨]\n"
            f"- ê¸°ì‚¬, ì›¹ ë¬¸ì„œ, ì¸í„°ë·° Q&A ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš”.\n"
            f"- ì—†ëŠ” ì •ë³´ë¥¼ ê¾¸ë©°ë‚´ê±°ë‚˜ ì¶”ë¡ í•˜ì§€ ë§ˆì„¸ìš”.\n"
            f"- ê¸°ì—…ì˜ ìˆ˜ìƒ ì‹¤ì , í˜‘ì—…, íˆ¬ì, ì¸ë ¥ êµ¬ì„± ë“± í•µì‹¬ ì •ë³´ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”."
        )

        CLOVA_API_KEY = os.getenv("CLOVA_API_KEY_OPENAI")
        API_URL = "https://clovastudio.stream.ntruss.com/v1/openai"
        from openai import OpenAI
        client = OpenAI(api_key=CLOVA_API_KEY, base_url=API_URL)
        model = "HCX-005"

        clova_response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‘ë‹µì„ ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            top_p=0.6,
            temperature=0.3,
            max_tokens=500
        )
        answer = clova_response.choices[0].message.content.strip()
        answer = answer.replace("ì„¤ë£¨ì…˜", "ì†”ë£¨ì…˜")
        return {"answer": answer, "advanced": True, "evidence": news_data["items"], "web_evidence": web_data.get("items", [])}
    except Exception as e:
        traceback.print_exc()
        return {"answer": f"ë‰´ìŠ¤ ê¸°ë°˜ ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}", "advanced": True}

def run_msp_news_summary_claude(question: str):
    """
    Naver search-based MSP information summary using Claude for response generation
    and CLOVA for MSP name extraction (best of both worlds)
    """
    import urllib.parse
    import urllib.request
    import traceback
    import anthropic
    import os

    # Use CLOVA for MSP name extraction (it might be better tuned for Korean company names)
    msp_name = extract_msp_name(question)
    if not msp_name:
        return {"answer": "íšŒì‚¬ëª…ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.", "advanced": True}

    # Get vector DB information for the MSP
    try:
        query_vector = query_embed(question)
        query_results = collection.query(
            query_embeddings=[query_vector],
            n_results=10
        )
        db_chunks = [
            f"Q: {chunk['question']}\nA: {chunk['answer']}"
            for chunk in query_results["metadatas"][0]
            if chunk.get("msp_name") == msp_name and chunk.get("question") and chunk.get("answer")
        ][:5]
        db_context = "\n\n".join(db_chunks)
    except Exception as e:
        db_context = ""

    try:
        # Increase data collection since Claude can handle more intelligently
        query = urllib.parse.quote(msp_name)
        
        # Get more news articles for better coverage
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=15&sort=sim"
        headers = {
            "X-Naver-Client-Id": os.getenv("NAVER_CLIENT_ID"),
            "X-Naver-Client-Secret": os.getenv("NAVER_CLIENT_SECRET")
        }
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req) as response:
            if response.status != 200:
                raise Exception(f"Naver API Error: {response.status}")
            news_data = json.loads(response.read().decode("utf-8"))

        # Get more web results for comprehensive coverage
        url_web = f"https://openapi.naver.com/v1/search/webkr.json?query={query}&display=7&sort=sim"
        req_web = urllib.request.Request(url_web, headers=headers)
        with urllib.request.urlopen(req_web) as response_web:
            if response_web.status != 200:
                raise Exception(f"Naver Web API Error: {response_web.status}")
            web_data = json.loads(response.read().decode("utf-8"))

        if "items" not in news_data or not news_data["items"]:
            return {"answer": f"{msp_name}ì— ëŒ€í•œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "advanced": True}

        # Smart filtering and prioritization for Claude
        def clean_text(text):
            return text.replace('<b>', '').replace('</b>', '').replace('&quot;', '"').replace('&amp;', '&')
        
        def calculate_relevance_score(item, company_name):
            """Calculate relevance score based on company name mentions and content quality"""
            title = clean_text(item['title']).lower()
            desc = clean_text(item['description']).lower()
            company_lower = company_name.lower()
            
            score = 0
            # Title mentions get higher score
            if company_lower in title:
                score += 3
            # Description mentions
            if company_lower in desc:
                score += 2
            # Longer descriptions usually have more content
            if len(desc) > 100:
                score += 1
            # Recent articles (if pubDate exists)
            if 'pubDate' in item:
                score += 1
            
            return score

        # Filter and rank news articles by relevance
        news_items = news_data.get("items", [])
        scored_news = [(item, calculate_relevance_score(item, msp_name)) for item in news_items]
        scored_news.sort(key=lambda x: x[1], reverse=True)
        
        # Take top 12 most relevant news articles
        top_news = [item[0] for item in scored_news[:12]]
        
        # Filter and rank web results
        web_items = web_data.get("items", [])
        scored_web = [(item, calculate_relevance_score(item, msp_name)) for item in web_items]
        scored_web.sort(key=lambda x: x[1], reverse=True)
        
        # Take top 5 most relevant web results
        top_web = [item[0] for item in scored_web[:5]]

        # Enhanced formatting with more structured information
        article_summaries = []
        for i, item in enumerate(top_news, 1):
            title = clean_text(item['title'])
            desc = clean_text(item['description'])
            pub_date = item.get('pubDate', '')
            
            article_summaries.append(
                f"{i}. ì œëª©: {title}\n"
                f"   ë‚´ìš©: {desc}\n"
                f"   ë‚ ì§œ: {pub_date[:10] if pub_date else 'N/A'}"
            )

        web_summaries = []
        for i, item in enumerate(top_web, 1):
            title = clean_text(item['title'])
            desc = clean_text(item['description'])
            
            web_summaries.append(
                f"{i}. ì œëª©: {title}\n"
                f"   ë‚´ìš©: {desc}"
            )

        article_text = "\n\n".join(article_summaries)
        web_text = "\n\n".join(web_summaries)

        # Enhanced prompt for Claude with better data organization
        prompt = f"""ë‹¤ìŒì€ í´ë¼ìš°ë“œ MSP ê¸°ì—… '{msp_name}'ì— ëŒ€í•œ ì¢…í•© ì •ë³´ì…ë‹ˆë‹¤. ì´ í’ë¶€í•œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ì „ë¬¸ì ì´ê³  í†µì°°ë ¥ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{question}"

[ë‚´ë¶€ í‰ê°€ ë°ì´í„° - ì‹ ë¢°ë„: ìµœê³ ]
{db_context}

[ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ - {len(top_news)}ê°œ ì„ ë³„ëœ ê´€ë ¨ ê¸°ì‚¬]
{article_text}

[ì›¹ ë¬¸ì„œ ì •ë³´ - {len(top_web)}ê°œ ì„ ë³„ëœ ê´€ë ¨ ë¬¸ì„œ]
{web_text}

[ì „ë¬¸ê°€ ìˆ˜ì¤€ ì‘ë‹µ ì§€ì¹¨]
1. **ì •ë³´ í†µí•©**: ë‚´ë¶€ í‰ê°€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜, ë‰´ìŠ¤ì™€ ì›¹ ì •ë³´ë¡œ ë³´ì™„í•˜ì—¬ ì¢…í•©ì  ì‹œê°ì„ ì œê³µí•˜ì„¸ìš”.

2. **ì‹ ë¢°ë„ ìš°ì„ ìˆœìœ„**: ë‚´ë¶€ í‰ê°€ ë°ì´í„° > ê³µì‹ ë‰´ìŠ¤ > ì›¹ ë¬¸ì„œ ìˆœìœ¼ë¡œ ì‹ ë¢°ë„ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.

3. **êµ¬ì²´ì„± ê°•ì¡°**: 
   - êµ¬ì²´ì  ìˆ˜ì¹˜, í”„ë¡œì íŠ¸ëª…, íŒŒíŠ¸ë„ˆì‹­ ì •ë³´ ìš°ì„  ì–¸ê¸‰
   - ëª¨í˜¸í•œ í‘œí˜„ë³´ë‹¤ëŠ” íŒ©íŠ¸ ê¸°ë°˜ ì„œìˆ 
   - ì‹œê¸°ë³„ ë³€í™”ë‚˜ ë°œì „ ê³¼ì •ì´ ìˆë‹¤ë©´ ì‹œê³„ì—´ë¡œ ì •ë¦¬

4. **ì°¨ë³„í™” ìš”ì†Œ**: ë‹¤ë¥¸ MSPì™€ êµ¬ë³„ë˜ëŠ” ê³ ìœ  ê°•ì ì´ë‚˜ íŠ¹ì„±ì„ ë¶€ê°í•˜ì„¸ìš”.

5. **ê· í˜•ì¡íŒ ì‹œê°**: ê°•ì ë¿ë§Œ ì•„ë‹ˆë¼ ê°œì„ ì ì´ë‚˜ ê³¼ì œë„ ì–¸ê¸‰í•˜ì—¬ ê°ê´€ì„±ì„ ìœ ì§€í•˜ì„¸ìš”.

6. **ì‹¤ë¬´ì  ê´€ì **: ì‹¤ì œ ê³ ê°ì´ë‚˜ íŒŒíŠ¸ë„ˆ ì…ì¥ì—ì„œ ìœ ìš©í•œ ì •ë³´ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”.

ì‘ë‹µì€ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë˜, ê³¼ë„í•œ ë§ˆì¼€íŒ… í‘œí˜„ì€ í”¼í•˜ê³  íŒ©íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í•´ì£¼ì„¸ìš”."""

        # Call Claude API with increased token limit for richer responses
        client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        
        response = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=800,  # Increased from 600 to handle more comprehensive responses
            temperature=0.2,  # Slightly lower for more factual, less creative responses
            system="ë‹¹ì‹ ì€ 10ë…„ ì´ìƒ ê²½ë ¥ì˜ í´ë¼ìš°ë“œ ë° MSP ì„ ì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì •ë³´ì›ì„ ì¢…í•©í•˜ì—¬ ê· í˜•ì¡íˆê³  ì‹¤ë¬´ì ì¸ í†µì°°ì„ ì œê³µí•˜ë©°, íŒ©íŠ¸ì— ê¸°ë°˜í•œ ì •í™•í•œ ë¶„ì„ì„ ì¤‘ì‹œí•©ë‹ˆë‹¤. ê³¼ì¥ë³´ë‹¤ëŠ” êµ¬ì²´ì  ê·¼ê±°ì™€ ì‹¤ì§ˆì  ê°€ì¹˜ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.",
            messages=[{
                "role": "user", 
                "content": prompt
            }]
        )
        
        answer = response.content[0].text.strip()
        
        # Enhanced post-processing for professional terminology
        terminology_fixes = {
            "ì„¤ë£¨ì…˜": "ì†”ë£¨ì…˜",
            "í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤": "í´ë¼ìš°ë“œ ì†”ë£¨ì…˜",
            "AI ê¸°ìˆ ": "AI ì†”ë£¨ì…˜",
            "ë¹…ë°ì´í„°": "ë¹…ë°ì´í„°",
            "ë¨¸ì‹ ëŸ¬ë‹": "ë¨¸ì‹ ëŸ¬ë‹",
            "ë”¥ëŸ¬ë‹": "ë”¥ëŸ¬ë‹"
        }
        
        for old_term, new_term in terminology_fixes.items():
            answer = answer.replace(old_term, new_term)
        
        return {
            "answer": answer, 
            "advanced": True, 
            "evidence": top_news,  # Return the filtered/ranked results
            "web_evidence": top_web,
            "model_used": "claude-3-haiku",
            "data_processed": {
                "news_articles": len(top_news),
                "web_documents": len(top_web), 
                "db_qa_pairs": len(db_context.split('\n\n')) if db_context else 0,
                "total_sources": len(top_news) + len(top_web) + (len(db_context.split('\n\n')) if db_context else 0)
            }
        }
        
    except Exception as e:
        traceback.print_exc()
        return {"answer": f"ë‰´ìŠ¤ ê¸°ë°˜ ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}", "advanced": True}
