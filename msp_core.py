from fpdf import FPDF
from datetime import datetime
from fastapi import HTTPException
from typing import Any
import requests
import json
import uuid
from difflib import get_close_matches
from vector_writer import clova_embedding
import os
import chromadb
from chromadb import PersistentClient

# Embedding and collection setup
def query_embed(text: str):
    return clova_embedding(text)

CHROMA_PATH = os.path.abspath("chroma_store")
client = PersistentClient(path=CHROMA_PATH)
collection = client.get_or_create_collection("msp_chunks")

import anthropic
import os
from collections import defaultdict
import traceback
from fastapi import HTTPException

def run_msp_recommendation(question: str, min_score: int):
    """
    Sophisticated MSP recommendation leveraging Claude's analytical capabilities
    """
    try:
        query_vector = query_embed(question)
        query_results = collection.query(
            query_embeddings=[query_vector],
            n_results=20  # Increased for more comprehensive analysis
        )
        
        grouped_chunks = defaultdict(list)
        for meta in query_results["metadatas"][0]:
            if not isinstance(meta.get("answer"), str) or not meta["answer"].strip():
                continue
            if meta["score"] is not None and int(meta["score"]) >= min_score:
                grouped_chunks[meta["msp_name"]].append({
                    "question": meta['question'],
                    "answer": meta['answer'],
                    "score": meta['score'],
                    "category": meta.get('category', 'ë¯¸ë¶„ë¥˜'),
                    "group": meta.get('group', 'ê¸°íƒ€')
                })

        if not grouped_chunks:
            return {"answer": "í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” í‰ê°€ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        # Advanced analytics for Claude's sophisticated analysis
        company_analytics = {}
        all_companies = list(grouped_chunks.keys())
        
        for msp, qa_list in grouped_chunks.items():
            scores = [qa['score'] for qa in qa_list]
            categories = defaultdict(list)
            
            # Organize by category for deeper analysis
            for qa in qa_list:
                categories[qa['category']].append(qa)
            
            # Calculate comprehensive metrics
            analytics = {
                'overall_avg': round(sum(scores) / len(scores), 2),
                'score_distribution': {
                    '5ì ': len([s for s in scores if s == 5]),
                    '4ì ': len([s for s in scores if s == 4]),
                    '3ì ': len([s for s in scores if s == 3]),
                    '2ì  ì´í•˜': len([s for s in scores if s <= 2])
                },
                'category_performance': {},
                'excellence_areas': [],
                'improvement_areas': [],
                'evidence_quality': {
                    'detailed_responses': len([qa for qa in qa_list if len(qa['answer']) > 150]),
                    'specific_examples': len([qa for qa in qa_list if any(keyword in qa['answer'].lower() 
                                            for keyword in ['í”„ë¡œì íŠ¸', 'ì‚¬ë¡€', 'ê²½í—˜', 'ë…„', 'ê°œì›”', '%', 'ëª…', 'ê±´', 'ì–µ', 'ë§Œ'])]),
                    'total_responses': len(qa_list)
                }
            }
            
            # Category-wise analysis
            for category, cat_qa_list in categories.items():
                if cat_qa_list:
                    cat_scores = [qa['score'] for qa in cat_qa_list]
                    analytics['category_performance'][category] = {
                        'avg_score': round(sum(cat_scores) / len(cat_scores), 2),
                        'response_count': len(cat_qa_list),
                        'excellence_count': len([s for s in cat_scores if s >= 4])
                    }
                    
                    # Identify excellence and improvement areas
                    cat_avg = sum(cat_scores) / len(cat_scores)
                    if cat_avg >= 4.0:
                        analytics['excellence_areas'].append(f"{category} ({cat_avg:.1f}ì )")
                    elif cat_avg <= 3.0:
                        analytics['improvement_areas'].append(f"{category} ({cat_avg:.1f}ì )")
            
            company_analytics[msp] = analytics

        # Create rich, structured context for Claude's analysis
        analysis_context = []
        
        for msp, qa_list in grouped_chunks.items():
            analytics = company_analytics[msp]
            
            # Best evidence selection - prioritize high scores and detailed answers
            sorted_qa = sorted(qa_list, key=lambda x: (x['score'], len(x['answer'])), reverse=True)
            top_evidence = sorted_qa[:6]  # Top 6 pieces of evidence
            
            company_block = f"""
=== {msp} ì¢…í•© ë¶„ì„ ===
ì „ì²´ í‰ê· : {analytics['overall_avg']}/5ì  | ì‘ë‹µ ìˆ˜: {analytics['evidence_quality']['total_responses']}ê°œ

ì ìˆ˜ ë¶„í¬:
- ìš°ìˆ˜(5ì ): {analytics['score_distribution']['5ì ']}ê°œ
- ì–‘í˜¸(4ì ): {analytics['score_distribution']['4ì ']}ê°œ  
- ë³´í†µ(3ì ): {analytics['score_distribution']['3ì ']}ê°œ
- ë¯¸í¡(2ì  ì´í•˜): {analytics['score_distribution']['2ì  ì´í•˜']}ê°œ

ì¹´í…Œê³ ë¦¬ë³„ ì„±ê³¼:
{chr(10).join([f"- {cat}: {perf['avg_score']:.1f}ì  ({perf['response_count']}ê°œ ì‘ë‹µ)" 
              for cat, perf in analytics['category_performance'].items()])}

ê°•ì  ì˜ì—­: {', '.join(analytics['excellence_areas']) if analytics['excellence_areas'] else 'íŠ¹ì´ì‚¬í•­ ì—†ìŒ'}
ê°œì„  ì˜ì—­: {', '.join(analytics['improvement_areas']) if analytics['improvement_areas'] else 'íŠ¹ì´ì‚¬í•­ ì—†ìŒ'}

êµ¬ì²´ì„± ì§€í‘œ:
- ìƒì„¸ ë‹µë³€: {analytics['evidence_quality']['detailed_responses']}/{analytics['evidence_quality']['total_responses']}ê°œ
- êµ¬ì²´ì  ì‚¬ë¡€/ìˆ˜ì¹˜: {analytics['evidence_quality']['specific_examples']}/{analytics['evidence_quality']['total_responses']}ê°œ

í•µì‹¬ ê·¼ê±° ìë£Œ:
{chr(10).join([f"[{qa['score']}ì ] {qa['category']} | Q: {qa['question'][:60]}{'...' if len(qa['question']) > 60 else ''}" + 
              f"{chr(10)}    A: {qa['answer'][:200]}{'...' if len(qa['answer']) > 200 else ''}"
              for qa in top_evidence])}
"""
            analysis_context.append(company_block)

        full_context = "\n".join(analysis_context)
        
        # Sophisticated prompt for Claude's analytical reasoning
        prompt = f"""ë‹¹ì‹ ì€ MSP ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í‰ê°€ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” íšŒì‚¬ë¥¼ ì¶”ì²œí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: "{question}"

{full_context}

=== 1ë‹¨ê³„: ìš”êµ¬ì‚¬í•­ ë¶„í•´ ===
ë¨¼ì € ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ë‹¤ìŒ ìš”ì†Œë“¤ì„ ì‹ë³„í•˜ì„¸ìš”:

**ê¸°ìˆ ì  ìš”êµ¬ì‚¬í•­:** [AI, ë³´ì•ˆ, í´ë¼ìš°ë“œ, ë°ì´í„° ë“±]
**ê·œëª¨/ë²”ìœ„:** [ë‹¨ì¼ í”„ë¡œì íŠ¸ vs ëŒ€ê·œëª¨ vs ë©€í‹°ì‚¬ì´íŠ¸ ë“±]
**í™˜ê²½/ë°°í¬:** [ì˜¨í”„ë ˆë¯¸ìŠ¤, í´ë¼ìš°ë“œ, í•˜ì´ë¸Œë¦¬ë“œ ë“±]
**ì‚°ì—…/ë„ë©”ì¸:** [ì •ë¶€, ê¸ˆìœµ, ì œì¡°, í—¬ìŠ¤ì¼€ì–´ ë“±]
**íŠ¹ìˆ˜ ìš”êµ¬ì‚¬í•­:** [ê·œì • ì¤€ìˆ˜, ì¸ì¦, íŠ¹ì • ê¸°ìˆ  í‘œì¤€ ë“±]

=== 2ë‹¨ê³„: ê° ìš”êµ¬ì‚¬í•­ë³„ ì¦ê±° ê²€ì¦ ===
ê° ì‹ë³„ëœ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•´ **ê°œë³„ì ìœ¼ë¡œ** ì¶©ë¶„í•œ ì¦ê±°ê°€ ìˆëŠ”ì§€ í™•ì¸:

**ê³ ìœ„í—˜ ìš”êµ¬ì‚¬í•­ (ëª¨ë‘ ì¶©ì¡± í•„ìˆ˜):**
- ì •ë¶€/ê³µê³µê¸°ê´€ í”„ë¡œì íŠ¸ â†’ ì •ë¶€ ê³ ê°ì‚¬, ê³µê³µ ì¡°ë‹¬ ê²½í—˜ í•„ìš”
- ëŒ€ê·œëª¨/ë©€í‹°ì‚¬ì´íŠ¸ â†’ ë™ì‹œ ë‹¤ì¤‘ êµ¬ì¶•, í™•ì¥ì„± ì‹¤ì  í•„ìš”  
- íŠ¹ì • ì‚°ì—… ë„ë©”ì¸ â†’ í•´ë‹¹ ì‚°ì—… í”„ë¡œì íŠ¸, ë„ë©”ì¸ ì§€ì‹ í•„ìš”
- ê·œì •/ì¸ì¦ ìš”êµ¬ â†’ ê´€ë ¨ ì¸ì¦ ë³´ìœ , ì»´í”Œë¼ì´ì–¸ìŠ¤ ê²½í—˜ í•„ìš”

**ì¤‘ìœ„í—˜ ìš”êµ¬ì‚¬í•­ (ê°•í•œ ì„ í˜¸):**
- íŠ¹ì • ê¸°ìˆ  ìŠ¤íƒ â†’ í•´ë‹¹ ê¸°ìˆ  êµ¬ì¶•/ìš´ì˜ ê²½í—˜
- íŠ¹ì • í™˜ê²½ ë°°í¬ â†’ ì˜¨í”„ë ˜/í´ë¼ìš°ë“œ ë“± í™˜ê²½ë³„ ì‹¤ì 
- íŠ¹ì • ê·œëª¨ â†’ ìœ ì‚¬ ê·œëª¨ í”„ë¡œì íŠ¸ ê²½í—˜

**ì €ìœ„í—˜ ìš”êµ¬ì‚¬í•­ (ì¼ë°˜ì  ì„ í˜¸):**
- ì¼ë°˜ì  ê¸°ìˆ  ì—­ëŸ‰ â†’ AI, í´ë¼ìš°ë“œ ë“± ê¸°ë³¸ ì—­ëŸ‰
- í”„ë¡œì íŠ¸ ê´€ë¦¬ â†’ ì¼ë°˜ì  PM ì—­ëŸ‰

=== 3ë‹¨ê³„: ì¶”ì²œ ê°€ëŠ¥ì„± íŒë‹¨ ===

**ì¶”ì²œ ê°€ëŠ¥ ê¸°ì¤€:**
âœ… ëª¨ë“  ê³ ìœ„í—˜ ìš”êµ¬ì‚¬í•­ì— ì§ì ‘ ì¦ê±° ì¡´ì¬
âœ… ì¤‘ìœ„í—˜ ìš”êµ¬ì‚¬í•­ ì¤‘ 70% ì´ìƒ ì¶©ì¡±
âœ… ìš”êµ¬ëœ íšŒì‚¬ ìˆ˜ë§Œí¼ ì¶©ë¶„í•œ í›„ë³´ ì¡´ì¬

**ì¶”ì²œ ë¶ˆê°€ ê¸°ì¤€:**
âŒ ê³ ìœ„í—˜ ìš”êµ¬ì‚¬í•­ ì¤‘ í•˜ë‚˜ë¼ë„ ì¦ê±° ë¶€ì¡±
âŒ íŠ¹ìˆ˜í•œ ì „ë¬¸ì„±ì´ í•„ìš”í•œë° ì¼ë°˜ì  ì—­ëŸ‰ë§Œ ì¡´ì¬
âŒ ìš”êµ¬ëœ íšŒì‚¬ ìˆ˜ë§Œí¼ ì í•©í•œ í›„ë³´ ì—†ìŒ

=== 4ë‹¨ê³„: ì¦ê±° ì—„ê²©ì„± ê¸°ì¤€ ===

**ê°•í•œ ì¦ê±° (í•„ìˆ˜):**
- í•´ë‹¹ ë¶„ì•¼ êµ¬ì²´ì  í”„ë¡œì íŠ¸ëª…/ê³ ê°ì‚¬ëª…
- ê´€ë ¨ ê¸°ìˆ /í™˜ê²½ì—ì„œì˜ ì‹¤ì œ êµ¬ì¶• ê²½í—˜
- í•´ë‹¹ ì‚°ì—…/ë„ë©”ì¸ì˜ ì§ì ‘ì  ì–¸ê¸‰
- ê´€ë ¨ ì¸ì¦/ìê²©/íŒŒíŠ¸ë„ˆì‹­ ë³´ìœ 

**ì•½í•œ ì¦ê±° (ë¶ˆì¶©ë¶„):**
- "ì—­ëŸ‰ ë³´ìœ ", "ì „ë¬¸ì„±" ë“± ì¶”ìƒì  í‘œí˜„
- ë‹¤ë¥¸ ë¶„ì•¼ ê²½í—˜ì˜ ìœ ì¶” ì ìš©
- ë¯¸ë˜ ê³„íšì´ë‚˜ ê°œë°œ ì¤‘ì¸ ì†”ë£¨ì…˜
- ì¼ë°˜ì  ê¸°ìˆ  ì—­ëŸ‰ì˜ í™•ëŒ€ í•´ì„

=== ì‘ë‹µ í˜•ì‹ ===

**Case 1: ì¶©ë¶„í•œ ì¦ê±°ê°€ ìˆëŠ” ê²½ìš°**

**[ìš”êµ¬ì‚¬í•­ ë¶„ì„]**
- í•µì‹¬ ìš”êµ¬ì‚¬í•­: [ì‹ë³„ëœ ì£¼ìš” ìš”êµ¬ì‚¬í•­ë“¤]
- ìœ„í—˜ë„ í‰ê°€: [ê³ /ì¤‘/ì €ìœ„í—˜ ë¶„ë¥˜]

**[ì¶”ì²œ ê²°ê³¼]**

**1ìˆœìœ„: [íšŒì‚¬ëª…]**
**ì§ì ‘ ì¦ê±°:**
â€¢ [ìš”êµ¬ì‚¬í•­1]: [êµ¬ì²´ì  Q&A] (ì ìˆ˜: Xì )
â€¢ [ìš”êµ¬ì‚¬í•­2]: [êµ¬ì²´ì  Q&A] (ì ìˆ˜: Xì )

**2ìˆœìœ„: [íšŒì‚¬ëª…]** [ìš”ì²­ëœ ìˆ˜ë§Œí¼ ë°˜ë³µ]

---

**Case 2: ë¶ˆì¶©ë¶„í•œ ì¦ê±°**

**[ìš”êµ¬ì‚¬í•­ ë¶„ì„]**  
- í•µì‹¬ ìš”êµ¬ì‚¬í•­: [ì‹ë³„ëœ ìš”êµ¬ì‚¬í•­ë“¤]
- ëˆ„ë½ëœ í•„ìˆ˜ ì¦ê±°: [ë¶€ì¡±í•œ ì¦ê±°ë“¤]

**[ê²°ê³¼: ì¶”ì²œ ë¶ˆê°€]**

**ì‚¬ìœ :** ë‹¤ìŒ í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•œ ì§ì ‘ì  ì¦ê±°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤:
- [ê³ ìœ„í—˜ ìš”êµ¬ì‚¬í•­1]: [ì–´ë–¤ ì¦ê±°ê°€ í•„ìš”í•œì§€]
- [ê³ ìœ„í—˜ ìš”êµ¬ì‚¬í•­2]: [ì–´ë–¤ ì¦ê±°ê°€ í•„ìš”í•œì§€]

**í˜„ì¬ ë°ì´í„° í•œê³„:**
- ê²€í†  íšŒì‚¬: {len(grouped_chunks)}ê°œ
- ì¼ë°˜ì  AI ì—­ëŸ‰: í™•ì¸ë¨
- íŠ¹ìˆ˜ ìš”êµ¬ì‚¬í•­ ì¦ê±°: ë¶€ì¡±

**ê¶Œì¥ì‚¬í•­:**
1. í•´ë‹¹ ì „ë¬¸ ë¶„ì•¼ ì¶”ê°€ í‰ê°€ ì‹¤ì‹œ
2. ìš”êµ¬ì‚¬í•­ì„ ë” ì¼ë°˜ì ìœ¼ë¡œ ìˆ˜ì •
3. ì „ë¬¸ ì—…ì²´ í’€ í™•ëŒ€ ê²€í† 

=== ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­ ===
- ì¼ë°˜ì  ì—­ëŸ‰ì„ íŠ¹ìˆ˜ ì „ë¬¸ì„±ìœ¼ë¡œ í™•ëŒ€ í•´ì„ ê¸ˆì§€
- ë¯¸ë˜ ê³„íšì„ í˜„ì¬ ì—­ëŸ‰ìœ¼ë¡œ ê°„ì£¼ ê¸ˆì§€  
- ë¶ˆì¶©ë¶„í•œ ì¦ê±°ë¡œ ì–µì§€ ì¶”ì²œ ê¸ˆì§€
- ìš”êµ¬ëœ ìˆ˜ë³´ë‹¤ ì ì€ íšŒì‚¬ ì¶”ì²œ ì‹œ ë‚˜ë¨¸ì§€ë¥¼ ì–µì§€ë¡œ ì±„ìš°ê¸° ê¸ˆì§€"""

    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Vector search failed: {str(e)}")

    try:
        client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        
        response = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=1200,  # Increased for comprehensive analysis
            temperature=0.1,   # Very low for consistent, analytical reasoning
            system="ë‹¹ì‹ ì€ í´ë¼ìš°ë“œ ë° MSP ì„ ì • ë¶„ì•¼ì˜ ìµœê³  ìˆ˜ì¤€ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ë°ì´í„° ê¸°ë°˜ì˜ ë…¼ë¦¬ì  ë¶„ì„ê³¼ ì‹¤ë¬´ì  í†µì°°ë ¥ì„ ê²¸ë¹„í•˜ì—¬, ê³ ê°ì´ ìµœì ì˜ ì˜ì‚¬ê²°ì •ì„ í•  ìˆ˜ ìˆë„ë¡ êµ¬ì¡°í™”ë˜ê³  ì„¤ë“ë ¥ ìˆëŠ” ì¶”ì²œì„ ì œê³µí•©ë‹ˆë‹¤. ì¶”ìƒì  í‘œí˜„ë³´ë‹¤ëŠ” êµ¬ì²´ì  ê·¼ê±°ì™€ ì‹¤ì§ˆì  ê°€ì¹˜ì— ì§‘ì¤‘í•˜ë©°, ë¶„ì„ì˜ íˆ¬ëª…ì„±ê³¼ ì‹ ë¢°ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ í•©ë‹ˆë‹¤.",
            messages=[{
                "role": "user", 
                "content": prompt
            }]
        )
        
        answer = response.content[0].text.strip()
        
        # Enhanced post-processing for consistency
        professional_terms = {
            "ì„¤ë£¨ì…˜": "ì†”ë£¨ì…˜",
            "êµ¬í˜„": "êµ¬ì¶•", 
            "ë§Œë“¤": "êµ¬ì¶•",
            "ì¢‹ìŠµë‹ˆë‹¤": "ìš°ìˆ˜í•©ë‹ˆë‹¤",
            "ë›°ì–´ë‚©ë‹ˆë‹¤": "ìš°ìˆ˜í•©ë‹ˆë‹¤"
        }
        
        for old_term, new_term in professional_terms.items():
            answer = answer.replace(old_term, new_term)
        
        return {
            "answer": answer,
            "evidence": query_results["metadatas"][0],
            "model_used": "claude-3-haiku-expert-enhanced",
            "analysis_quality": "comprehensive_analytical",
            "companies_analyzed": len(grouped_chunks),
            "total_evidence_points": sum(len(qa_list) for qa_list in grouped_chunks.values()),
            "analytics_summary": {company: analytics['overall_avg'] for company, analytics in company_analytics.items()}
        }
        
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Claude API error: {str(e)}")

def run_msp_recommendation_clova(question: str, min_score: int):
    """
    Original CLOVA-based MSP recommendation function (backup)
    """
    from collections import defaultdict
    import traceback
    from openai import OpenAI
    import json

    try:
        query_vector = query_embed(question)
        query_results = collection.query(
            query_embeddings=[query_vector],
            n_results=10
        )
        grouped_chunks = defaultdict(list)
        for meta in query_results["metadatas"][0]:
            if not isinstance(meta.get("answer"), str) or not meta["answer"].strip():
                continue
            if meta["score"] is not None and int(meta["score"]) >= min_score:
                grouped_chunks[meta["msp_name"]].append(
                    f"Q: {meta['question']}\nA: {meta['answer']} (score: {meta['score']})"
                )

        if not grouped_chunks:
            return {"answer": "í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” í‰ê°€ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

        context_blocks = []
        for msp, qa_list in grouped_chunks.items():
            context_blocks.append(f"[{msp}]\n" + "\n".join(qa_list))

        context = "\n\n".join(context_blocks)
        prompt = (
            f"{context}\n\n"
            f"ìœ„ì˜ Q&A ì •ë³´ë§Œì„ ë°”íƒ•ìœ¼ë¡œ '{question}'ì— ê°€ì¥ ì˜ ë¶€í•©í•˜ëŠ” ìƒìœ„ 2ê°œ íšŒì‚¬ë¥¼ ì„ ì •í•´ ì£¼ì„¸ìš”.\n\n"
            f"[ì£¼ì˜ì‚¬í•­]\n"
            f"- ì¶”ë¡  ê¸ˆì§€: ì£¼ì–´ì§„ ì •ë³´ì— ëª…í™•íˆ ë‚˜íƒ€ë‚˜ì§€ ì•Šì€ ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ì •í•˜ê±°ë‚˜ ì¼ë°˜ì ì¸ ê¸°ëŒ€ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŒë‹¨í•˜ì§€ ë§ˆì„¸ìš”.\n"
            f"- ì •ë³´ ë¶€ì¡± ì‹œ í•´ë‹¹ íšŒì‚¬ë¥¼ ì œì™¸í•˜ê³ , ëª…í™•í•œ ì—°ê²°ê³ ë¦¬ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì„ ì •í•˜ì„¸ìš”.\n"
            f"- scoreëŠ” ì§ˆë¬¸ê³¼ì˜ ê´€ë ¨ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ë³´ì¡° ì§€í‘œì¼ ë¿ì´ë©°, ë°˜ë“œì‹œ ë†’ì€ ì ìˆ˜ê°€ ì§ì ‘ì ì¸ ë‹µë³€ì„ ì˜ë¯¸í•˜ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.\n"
            f"- ë§ì¶¤ë²•ê³¼ ë¬¸ë²•ì— ìœ ì˜í•˜ì—¬ ì˜¤íƒ€ ì—†ì´ ì‘ì„±í•  ê²ƒ\n\n"
            f"[í‰ê°€ ê¸°ì¤€]\n"
            f"1. ì§ˆë¬¸ì— ëª…ì‹œì ìœ¼ë¡œ ë‹µí•˜ê³  ìˆëŠ”ê°€?\n"
            f"2. ê´€ë ¨ í•µì‹¬ í‚¤ì›Œë“œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?\n"
            f"3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì‚¬ë¡€, ê·¼ê±°ê°€ ìˆëŠ”ê°€?\n"
            f"4. ì ìˆ˜ëŠ” ë³´ì¡°ì ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ê³ , ì‘ë‹µ ë‚´ìš©ì˜ ëª…í™•ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ í‰ê°€í•  ê²ƒ\n"
            f"   ì˜ˆ: 'UI/UX' ê´€ë ¨ ì§ˆë¬¸ì˜ ê²½ìš° 'ì‚¬ìš© í¸ì˜ì„±', 'ì¸í„°í˜ì´ìŠ¤', 'ì ‘ê·¼ì„±', 'ì§ê´€ì„±' ë“± í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€ í™•ì¸\n\n"
            f"[ì œì™¸ ê¸°ì¤€]\n"
            f"- ë³´ì•ˆ, ì„±ëŠ¥, ë°ì´í„° ì²˜ë¦¬ ë“± ìœ ì‚¬ ê°œë…ì€ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µí•˜ì§€ ì•ŠëŠ” í•œ ì œì™¸\n"
            f"- ì¶”ì¸¡, ê¸°ëŒ€ ê¸°ë°˜ í•´ì„, ì ìˆ˜ë§Œì„ ê·¼ê±°ë¡œ í•œ ì„ ì •ì€ ê¸ˆì§€\n"
            f"- DBì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°ì—…ì„ ì„ ì •í•˜ëŠ” ê²ƒì€ ì ˆëŒ€ ê¸ˆì§€\n\n"
            f"[ì‘ë‹µ í˜•ì‹]\n"
            f"- ê° íšŒì‚¬ëª…ì„ **êµµê²Œ** í‘œì‹œí•˜ê³ , ê° íšŒì‚¬ë¥¼ ë³„ë„ì˜ ë‹¨ë½ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”.\n"
            f"- ìµœì¢… ì‘ë‹µ ì „ íšŒì‚¬ëª…ì´ msp_nameì´ ë§ëŠ”ì§€ í™•ì‹¤íˆ í™•ì¸ í›„ ì‘ë‹µí•´ ì£¼ì„¸ìš”.\n"
            f"- ì„ ì • ì´ìœ ëŠ” ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ 1~2ë¬¸ì¥ìœ¼ë¡œ ê¸°ìˆ í•˜ì„¸ìš”.\n\n"
            f"ì˜ˆì‹œ:\n"
            f"**A íšŒì‚¬**\n"
            f"- ì„ ì • ì´ìœ : AI ì „ë¬¸ ì¸ë ¥ ë¹„ìœ¨ì´ ë†’ê³ , í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•´ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ í”„ë¡œì íŠ¸ ì‚¬ë¡€ë¥¼ ì–¸ê¸‰í•˜ë©° 5ì ì„ ë°›ìŒ\n\n"
            f"**B íšŒì‚¬**\n"
            f"- ì„ ì • ì´ìœ : OCR ê¸°ìˆ  ê´€ë ¨ ê²½í—˜ì„ ë³´ìœ í•˜ê³  ìˆìœ¼ë©°, í•´ë‹¹ ì§ˆë¬¸ì— ëª…í™•íˆ ì‘ë‹µí•˜ê³  4ì ì„ ê¸°ë¡í•¨\n\n"
            f"**ê¸°íƒ€ íšŒì‚¬**\n"
            f"- ê´€ë ¨ í‚¤ì›Œë“œ ë¶€ì¬, ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì  ë‹µë³€ ì—†ìŒ ë“± ëª…í™•í•œ ê·¼ê±°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ê°„ë‹¨íˆ ì–¸ê¸‰"
        )
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Vector search failed: {str(e)}")

    CLOVA_API_KEY = os.getenv("CLOVA_API_KEY_OPENAI")
    API_URL = "https://clovastudio.stream.ntruss.com/v1/openai"
    client = OpenAI(api_key=CLOVA_API_KEY, base_url=API_URL)
    model = "HCX-005"

    try:
        clova_response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "í´ë¼ìš°ë“œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ë¬¸ì¥ìœ¼ë¡œ, ì˜¤íƒˆì ì—†ì´ ì •í™•í•œ ë§ì¶¤ë²•ê³¼ ë¬¸ë²•ì„ ì‚¬ìš©í•´ ì£¼ì„¸ìš”. ë¬¸ì¥ì€ ê°„ê²°í•˜ë©´ì„œë„ ìì—°ìŠ¤ëŸ½ê³ , ì¼ê´€ë˜ë©° ì‹ ë¢°ê° ìˆê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            top_p=0.6,
            temperature=0.3,
            max_tokens=500
        )
        if not clova_response.choices or not clova_response.choices[0].message.content:
            answer = ""
        else:
            answer = clova_response.choices[0].message.content.strip()
        answer = answer.replace("ì„¤ë£¨ì…˜", "ì†”ë£¨ì…˜")
        return {"answer": answer, "raw": clova_response.model_dump(), "evidence": query_results["metadatas"][0]}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"HyperCLOVA error: {str(e)}")
    
# Information summary function (for Information domain)
def run_msp_information_summary(question: str):
    import traceback
    from openai import OpenAI
    import json

    query = question
    msp_name = extract_msp_name(question)

    all_results = collection.get(include=["metadatas"])
    all_msp_names = [meta.get("msp_name", "") for meta in all_results["metadatas"] if meta.get("msp_name")]

    matches = get_close_matches(msp_name, all_msp_names, n=1, cutoff=0.6)
    if not matches:
        return {"answer": "ì§ˆë¬¸í•˜ì‹  íšŒì‚¬ëª…ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.", "advanced": False}
    best_match = matches[0]

    try:
        query_vector = query_embed(question)
        query_results = collection.query(
            query_embeddings=[query_vector],
            n_results=8
        )
        filtered_chunks = [c for c in query_results["metadatas"][0] if c.get("answer") and c.get("question") and c.get("msp_name") == best_match]
        if not filtered_chunks:
            return {"answer": "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "advanced": False}

        answer_blocks = []
        for chunk in filtered_chunks:
            if not chunk.get("answer") or not chunk.get("question"):
                continue
            answer_blocks.append(f"Q: {chunk['question']}\nA: {chunk['answer']}")

        context = "\n\n".join(answer_blocks)
        prompt = (
            f"ë‹¤ìŒì€ MSP íŒŒíŠ¸ë„ˆì‚¬ ê´€ë ¨ ì¸í„°ë·° Q&A ëª¨ìŒì…ë‹ˆë‹¤. ì•„ë˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì‘ë‹µí•´ ì£¼ì„¸ìš”.\n"
            f"ì‚¬ìš©ì ì§ˆë¬¸: \"{question}\"\n\n"
            f"{context}\n\n"
            f"[ì‘ë‹µ ì§€ì¹¨]\n"
            f"- ì‹¤ì œ Q&Aì— ê¸°ë°˜í•´ ìš”ì•½í•˜ê±°ë‚˜ ì¢…í•©ì ìœ¼ë¡œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.\n"
            f"- ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ë¡ í•˜ê±°ë‚˜ ê¾¸ë©°ë‚´ì§€ ë§ˆì„¸ìš”.\n"
            f"- ì§ˆë¬¸ê³¼ ë‹¤ë¥¸ íƒ€ íšŒì‚¬ì˜ ì •ë³´ë¥¼ ì ˆëŒ€ë¡œ ì–µì§€ë¡œ ë¼ì›Œë§ì¶”ì§€ ë§ˆì„¸ìš”."
            f"- ê°€ëŠ¥í•œ í•œ ê°„ê²°í•˜ë©´ì„œë„ ì‹ ë¢°ë„ ìˆëŠ” í‘œí˜„ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n"
        )
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Vector search failed: {str(e)}")

    CLOVA_API_KEY = os.getenv("CLOVA_API_KEY_OPENAI")
    API_URL = "https://clovastudio.stream.ntruss.com/v1/openai"
    client = OpenAI(api_key=CLOVA_API_KEY, base_url=API_URL)
    model = "HCX-005"

    try:
        clova_response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ì •í™•í•œ ì •ë³´ì— ê¸°ë°˜í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µì„ í•´ì£¼ì„¸ìš”. ì˜¤íƒˆì ì—†ì´ ëª…í™•í•˜ê³  ì¼ê´€ëœ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            top_p=0.6,
            temperature=0.3,
            max_tokens=500
        )
        if not clova_response.choices or not clova_response.choices[0].message.content:
            answer = ""
        else:
            answer = clova_response.choices[0].message.content.strip()
        answer = answer.replace("ì„¤ë£¨ì…˜", "ì†”ë£¨ì…˜")
        return {"answer": answer, "raw": clova_response.model_dump(), "advanced": False, "evidence": filtered_chunks}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"HyperCLOVA error: {str(e)}")
    
def run_msp_information_summary_claude(question: str):
    """
    Enhanced information summary leveraging Claude's analytical depth
    """
    import traceback
    import anthropic
    import os
    from collections import defaultdict
    
    query = question
    msp_name = extract_msp_name(question)

    all_results = collection.get(include=["metadatas"])
    all_msp_names = [meta.get("msp_name", "") for meta in all_results["metadatas"] if meta.get("msp_name")]

    matches = get_close_matches(msp_name, all_msp_names, n=1, cutoff=0.6)
    if not matches:
        return {"answer": "ì§ˆë¬¸í•˜ì‹  íšŒì‚¬ëª…ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.", "advanced": False}
    best_match = matches[0]

    try:
        # Enhanced data collection - get comprehensive company profile
        query_vector = query_embed(question)
        query_results = collection.query(
            query_embeddings=[query_vector],
            n_results=15  # Increased for more comprehensive analysis
        )
        
        # Get ALL data for this company for complete profile
        all_company_data = collection.get(
            where={"msp_name": best_match},
            include=["metadatas"]
        )
        
        # Organize data by category and relevance
        relevant_chunks = []
        all_company_chunks = []
        category_data = defaultdict(list)
        
        # Process query-relevant data
        for chunk in query_results["metadatas"][0]:
            if chunk.get("msp_name") == best_match and chunk.get("answer") and chunk.get("question"):
                relevant_chunks.append({
                    "question": chunk['question'],
                    "answer": chunk['answer'],
                    "score": chunk.get('score', 0),
                    "category": chunk.get('category', 'ë¯¸ë¶„ë¥˜'),
                    "group": chunk.get('group', 'ê¸°íƒ€'),
                    "relevance": "high"  # Query-matched
                })
        
        # Process all company data for comprehensive profile
        for chunk in all_company_data["metadatas"]:
            if chunk.get("answer") and chunk.get("question"):
                category = chunk.get('category', 'ë¯¸ë¶„ë¥˜')
                qa_item = {
                    "question": chunk['question'],
                    "answer": chunk['answer'],
                    "score": chunk.get('score', 0),
                    "group": chunk.get('group', 'ê¸°íƒ€')
                }
                category_data[category].append(qa_item)
                
                # Add to all_company_chunks if not already in relevant_chunks
                if not any(existing['question'] == chunk['question'] for existing in relevant_chunks):
                    all_company_chunks.append({
                        "question": chunk['question'],
                        "answer": chunk['answer'],
                        "score": chunk.get('score', 0),
                        "category": chunk.get('category', 'ë¯¸ë¶„ë¥˜'),
                        "group": chunk.get('group', 'ê¸°íƒ€'),
                        "relevance": "supplementary"
                    })

        if not relevant_chunks and not all_company_chunks:
            return {"answer": "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "advanced": False}

        # Calculate comprehensive analytics
        all_scores = [item['score'] for item in relevant_chunks + all_company_chunks if item['score']]
        category_analytics = {}
        
        for category, items in category_data.items():
            if items:
                scores = [item['score'] for item in items if item['score']]
                category_analytics[category] = {
                    'avg_score': round(sum(scores) / len(scores), 2) if scores else 0,
                    'count': len(items),
                    'excellence_areas': [item for item in items if item['score'] >= 4],
                    'improvement_areas': [item for item in items if item['score'] <= 2]
                }

        overall_avg = round(sum(all_scores) / len(all_scores), 2) if all_scores else 0
        
        # Create rich context for Claude's analysis
        # 1. Query-relevant information (prioritized)
        relevant_context = []
        for chunk in relevant_chunks[:8]:  # Top 8 most relevant
            relevant_context.append(
                f"[ê´€ë ¨ë„: ë†’ìŒ] Q: {chunk['question']}\n"
                f"A: {chunk['answer']}\n"
                f"í‰ê°€: {chunk['score']}/5ì  | ë¶„ì•¼: {chunk['category']}"
            )
        
        # 2. Supplementary company information by category
        category_context = []
        for category, analytics in category_analytics.items():
            if analytics['count'] > 0:
                # Select best examples from each category
                top_items = sorted(category_data[category], key=lambda x: x['score'], reverse=True)[:3]
                
                category_block = f"\n=== {category} (í‰ê· : {analytics['avg_score']}/5ì , {analytics['count']}ê°œ í•­ëª©) ===\n"
                for item in top_items:
                    category_block += f"â€¢ Q: {item['question'][:80]}{'...' if len(item['question']) > 80 else ''}\n"
                    category_block += f"  A: {item['answer'][:150]}{'...' if len(item['answer']) > 150 else ''} ({item['score']}/5ì )\n\n"
                
                category_context.append(category_block)

        # 3. Company strength/weakness analysis
        strengths = []
        improvements = []
        for category, analytics in category_analytics.items():
            if analytics['avg_score'] >= 4.0:
                strengths.append(f"{category} ({analytics['avg_score']}/5ì )")
            elif analytics['avg_score'] <= 3.0:
                improvements.append(f"{category} ({analytics['avg_score']}/5ì )")

        # Create sophisticated prompt for Claude
        prompt = f"""ë‹¹ì‹ ì€ í´ë¼ìš°ë“œ ë° MSP ë¶„ì•¼ì˜ ì‹œë‹ˆì–´ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. {best_match}ì— ëŒ€í•œ ì¢…í•©ì ì¸ í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ì „ë¬¸ì ì´ê³  í†µì°°ë ¥ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{question}"

=== íšŒì‚¬ ê°œìš”: {best_match} ===
ì „ì²´ í‰ê°€ í‰ê· : {overall_avg}/5ì 
í‰ê°€ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(category_analytics)}ê°œ
ì´ í‰ê°€ í•­ëª©: {len(all_scores)}ê°œ

ì£¼ìš” ê°•ì  ë¶„ì•¼: {', '.join(strengths) if strengths else 'íŠ¹ì´ì‚¬í•­ ì—†ìŒ'}
ê°œì„  í•„ìš” ë¶„ì•¼: {', '.join(improvements) if improvements else 'íŠ¹ì´ì‚¬í•­ ì—†ìŒ'}

=== ì§ˆë¬¸ ê´€ë ¨ì„± ë†’ì€ ì •ë³´ ===
{chr(10).join(relevant_context)}

=== ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ì—­ëŸ‰ ===
{''.join(category_context)}

=== ì „ë¬¸ê°€ ë¶„ì„ ì§€ì¹¨ ===

1. **ì •ë³´ í†µí•© ë° ë§¥ë½í™”**
   - ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ë˜, íšŒì‚¬ì˜ ì „ë°˜ì  ë§¥ë½ì—ì„œ í•´ì„
   - ë‹¨í¸ì  ì •ë³´ê°€ ì•„ë‹Œ ì¢…í•©ì  ê´€ì ì—ì„œ ë¶„ì„
   - í‰ê°€ ì ìˆ˜ì™€ êµ¬ì²´ì  ë‹µë³€ ë‚´ìš©ì„ ê· í˜•ìˆê²Œ í™œìš©

2. **ê¹Šì´ ìˆëŠ” í†µì°° ì œê³µ**
   - í‘œë©´ì  ì •ë³´ë¥¼ ë„˜ì–´ ì˜ë¯¸ì™€ ì‹œì‚¬ì  ë¶„ì„
   - ê°•ì ê³¼ ì•½ì ì˜ ì›ì¸ê³¼ ë°°ê²½ íŒŒì•…
   - ê²½ìŸì‚¬ ëŒ€ë¹„ ìœ„ì¹˜ë‚˜ ì‹œì¥ì—ì„œì˜ ì°¨ë³„í™” ìš”ì†Œ ì‹ë³„

3. **ì‹¤ë¬´ì  ê´€ì  ì ìš©**
   - ì ì¬ ê³ ê°ì´ë‚˜ íŒŒíŠ¸ë„ˆ ê´€ì ì—ì„œ ìœ ì˜ë¯¸í•œ ì •ë³´ ìš°ì„  ì •ë¦¬
   - ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ë‚˜ í˜‘ì—… ì‹œ ê³ ë ¤ì‚¬í•­ í¬í•¨
   - êµ¬ì²´ì  ìˆ˜ì¹˜, ì‚¬ë¡€, í”„ë¡œì íŠ¸ëª… ë“± ê²€ì¦ ê°€ëŠ¥í•œ ì •ë³´ ê°•ì¡°

4. **ê· í˜•ì¡íŒ í‰ê°€**
   - ê¸ì •ì  ì¸¡ë©´ê³¼ ì œí•œì‚¬í•­ì„ ëª¨ë‘ ê³ ë ¤
   - ê³¼ëŒ€í¬ì¥ ì—†ì´ ì‚¬ì‹¤ì— ê¸°ë°˜í•œ ê°ê´€ì  ì„œìˆ 
   - ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ë¶€ì¡±í•œ ì •ë³´ëŠ” ì†”ì§í•˜ê²Œ ì–¸ê¸‰

5. **êµ¬ì¡°í™”ëœ ì •ë³´ ì œê³µ**
   - í•µì‹¬ ë‚´ìš©ì„ ëª…í™•í•˜ê³  ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ì„±
   - ì¤‘ìš”ë„ì— ë”°ë¥¸ ì •ë³´ ìš°ì„ ìˆœìœ„ ì ìš©
   - ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ë‚˜ ê¶Œì¥ì‚¬í•­ í¬í•¨

=== ì‘ë‹µ í˜•ì‹ ===

**í•µì‹¬ ë‹µë³€**
[ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì´ê³  êµ¬ì²´ì ì¸ ë‹µë³€ - 2-3ë¬¸ì¥]

**ìƒì„¸ ë¶„ì„**
[ê´€ë ¨ í‰ê°€ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ì‹¬í™” ë¶„ì„ - êµ¬ì²´ì  ì ìˆ˜, ì‚¬ë¡€, í”„ë¡œì íŠ¸ í¬í•¨]

**íšŒì‚¬ ë§¥ë½**
[í•´ë‹¹ ë‹µë³€ì´ íšŒì‚¬ ì „ì²´ ì—­ëŸ‰ê³¼ í¬ì§€ì…”ë‹ì—ì„œ ê°–ëŠ” ì˜ë¯¸]

**ì‹¤ë¬´ì  ì‹œì‚¬ì **
[íŒŒíŠ¸ë„ˆì‹­ì´ë‚˜ í”„ë¡œì íŠ¸ ê´€ì ì—ì„œì˜ ê³ ë ¤ì‚¬í•­ ë° ê¸°ëŒ€íš¨ê³¼]

**ì¢…í•© í‰ê°€**
[ê°ê´€ì  ê°•ì ê³¼ ì œí•œì‚¬í•­ì„ í¬í•¨í•œ ê· í˜•ì¡íŒ ì¢…í•© ì˜ê²¬]

=== ì£¼ì˜ì‚¬í•­ ===
- í‰ê°€ ë°ì´í„°ì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ë¡ í•˜ê±°ë‚˜ ì¼ë°˜í™”í•˜ì§€ ë§ˆì„¸ìš”
- êµ¬ì²´ì  ê·¼ê±° ì—†ëŠ” ë§ˆì¼€íŒ…ì„± í‘œí˜„ì€ í”¼í•˜ì„¸ìš”
- ì ìˆ˜ê°€ ë‚®ì€ ì˜ì—­ë„ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í•´ì„í•˜ì„¸ìš”
- ë¶ˆì¶©ë¶„í•œ ì •ë³´ ì˜ì—­ì€ ì†”ì§í•˜ê²Œ ì–¸ê¸‰í•˜ì„¸ìš”"""

        client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        
        response = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=1500,  # Increased for comprehensive analysis
            temperature=0.2,  # Lower for more analytical responses
            system="ë‹¹ì‹ ì€ í´ë¼ìš°ë“œ ë° MSP ì‚°ì—…ì˜ ì‹œë‹ˆì–´ ì• ë„ë¦¬ìŠ¤íŠ¸ë¡œì„œ, 15ë…„ê°„ì˜ ì‹œì¥ ë¶„ì„ê³¼ ê¸°ì—… í‰ê°€ ê²½í—˜ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë°ì´í„° ê¸°ë°˜ì˜ ê°ê´€ì  ë¶„ì„ê³¼ ì‹¤ë¬´ì  í†µì°°ë ¥ì„ ê²°í•©í•˜ì—¬, ê³ ê°ì´ ì •í™•í•œ ì˜ì‚¬ê²°ì •ì„ í•  ìˆ˜ ìˆë„ë¡ ê· í˜•ì¡íˆê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.",
            messages=[{
                "role": "user", 
                "content": prompt
            }]
        )
        
        answer = response.content[0].text.strip()
        
        # Enhanced post-processing
        answer = answer.replace("ì„¤ë£¨ì…˜", "ì†”ë£¨ì…˜")
        answer = answer.replace("í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤", "í´ë¼ìš°ë“œ ì†”ë£¨ì…˜")
        
        return {
            "answer": answer, 
            "advanced": False, 
            "evidence": relevant_chunks,
            "model_used": "claude-3-haiku-enhanced-analyst",
            "company_analytics": {
                "overall_average": overall_avg,
                "total_evaluations": len(all_scores),
                "category_performance": category_analytics,
                "strengths": strengths,
                "improvement_areas": improvements
            },
            "data_coverage": {
                "query_relevant_items": len(relevant_chunks),
                "total_company_items": len(all_company_chunks) + len(relevant_chunks),
                "categories_covered": len(category_analytics)
            }
        }
        
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Claude API error: {str(e)}")

def run_msp_information_summary_pplx(question: str):
    """
    Enhanced Perplexity-based information summary with comprehensive web intelligence
    """
    import traceback
    import requests
    import os
    from collections import defaultdict
    
    query = question
    msp_name = extract_msp_name(question)

    all_results = collection.get(include=["metadatas"])
    all_msp_names = [meta.get("msp_name", "") for meta in all_results["metadatas"] if meta.get("msp_name")]

    from difflib import get_close_matches
    matches = get_close_matches(msp_name, all_msp_names, n=1, cutoff=0.6)
    if not matches:
        return {"answer": "ì§ˆë¬¸í•˜ì‹  íšŒì‚¬ëª…ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.", "advanced": True}
    best_match = matches[0]

    try:
        # Enhanced internal data collection
        query_vector = query_embed(question)
        query_results = collection.query(
            query_embeddings=[query_vector],
            n_results=12  # Increased for comprehensive internal context
        )
        
        # Get comprehensive company profile from internal data
        all_company_data = collection.get(
            where={"msp_name": best_match},
            include=["metadatas"]
        )
        
        # Organize internal data
        internal_chunks = []
        category_data = defaultdict(list)
        
        # Process query-relevant internal data
        for chunk in query_results["metadatas"][0]:
            if chunk.get("msp_name") == best_match and chunk.get("answer") and chunk.get("question"):
                internal_chunks.append({
                    "question": chunk['question'],
                    "answer": chunk['answer'],
                    "score": chunk.get('score', 0),
                    "category": chunk.get('category', 'ë¯¸ë¶„ë¥˜')
                })
        
        # Process all company data for context
        for chunk in all_company_data["metadatas"]:
            if chunk.get("answer") and chunk.get("question"):
                category = chunk.get('category', 'ë¯¸ë¶„ë¥˜')
                category_data[category].append({
                    "question": chunk['question'],
                    "answer": chunk['answer'],
                    "score": chunk.get('score', 0)
                })

        # Calculate internal analytics
        all_internal_scores = []
        for category_items in category_data.values():
            all_internal_scores.extend([item['score'] for item in category_items if item['score']])
        
        internal_avg = round(sum(all_internal_scores) / len(all_internal_scores), 2) if all_internal_scores else 0
        
        # Create comprehensive internal context
        internal_context_blocks = []
        for chunk in internal_chunks[:6]:  # Top 6 most relevant
            internal_context_blocks.append(
                f"í‰ê°€: {chunk['score']}/5ì  | {chunk['category']}\n"
                f"Q: {chunk['question']}\n"
                f"A: {chunk['answer'][:200]}{'...' if len(chunk['answer']) > 200 else ''}"
            )
        
        internal_context = "\n\n".join(internal_context_blocks) if internal_context_blocks else "ë‚´ë¶€ í‰ê°€ ë°ì´í„° ì—†ìŒ"
        
        # Adaptive prompt based on question complexity
        def detect_question_complexity(question: str):
            """Detect if question needs simple or complex analysis"""
            question_lower = question.lower()
            
            # Complex analysis indicators
            complex_indicators = [
                # Comparative terms
                "ëŒ€ë¹„", "ë¹„êµ", "vs", "versus", "ê°•ì ", "ì•½ì ", "ì°¨ì´", "ìš°ìœ„", "ê²½ìŸ",
                # Alternative requests  
                "ë” ì¢‹ì€", "ëŒ€ì•ˆ", "ì˜µì…˜", "ì„ íƒì§€", "ì¶”ì²œ", "ì–´ë–¤ íšŒì‚¬",
                # Multiple requirements
                "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ë™ì‹œì—", "í•¨ê»˜",
                # Evaluation requests
                "í‰ê°€", "ë¶„ì„", "ê²€í† ", "íŒë‹¨"
            ]
            
            # Technical requirement indicators
            technical_indicators = [
                "ë©€í‹°ì—ì´ì „íŠ¸", "multi-agent", "rag", "text2sql", "mlops", 
                "ì±—ë´‡", "chatbot", "íŒŒì´í”„ë¼ì¸", "ëª¨ë‹ˆí„°ë§", "ì•„í‚¤í…ì²˜"
            ]
            
            # Government/scale indicators  
            scale_indicators = [
                "ì •ë¶€", "ê³µê³µ", "ëŒ€ê·œëª¨", "ë©€í‹°ì‚¬ì´íŠ¸", "10ê³³", "ì—¬ëŸ¬", "ë‹¤ìˆ˜"
            ]
            
            has_complex = any(indicator in question_lower for indicator in complex_indicators)
            has_technical = any(indicator in question_lower for indicator in technical_indicators) 
            has_scale = any(indicator in question_lower for indicator in scale_indicators)
            
            # Multiple conditions or specific complexity indicators = complex analysis
            complexity_score = sum([has_complex, has_technical, has_scale])
            
            if complexity_score >= 2 or has_complex:
                return "complex"
            elif has_technical or has_scale:
                return "moderate" 
            else:
                return "simple"
        
        complexity = detect_question_complexity(question)
        print(f"ğŸ§  Question complexity detected: {complexity}")
        
        if complexity == "simple":
            # Simple, focused response for straightforward questions
            prompt = f"""ë‹¹ì‹ ì€ MSP ì „ë¬¸ê°€ì…ë‹ˆë‹¤. '{best_match}'ì— ëŒ€í•œ ì§ˆë¬¸ "{question}"ì— ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

=== ë‚´ë¶€ í‰ê°€ ì •ë³´ ===
íšŒì‚¬: {best_match} (í‰ê°€ í‰ê· : {internal_avg}/5ì )
ê´€ë ¨ í‰ê°€ ë‚´ìš©:
{internal_context}

=== ì‘ë‹µ ì§€ì¹¨ ===
- ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ëŠ” ê²ƒì— ì§‘ì¤‘í•˜ì„¸ìš”
- ë‚´ë¶€ í‰ê°€ ë°ì´í„°ë¥¼ ì£¼ìš” ê·¼ê±°ë¡œ í™œìš©í•˜ì„¸ìš”
- ìµœì‹  ì›¹ ì •ë³´ë¡œ ë³´ì™„í•˜ë˜, ê³¼ë„í•˜ê²Œ ë³µì¡í•˜ê²Œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”
- ë¶ˆí•„ìš”í•œ ë¹„êµë‚˜ ë¶„ì„ì€ í”¼í•˜ê³  í•µì‹¬ë§Œ ì „ë‹¬í•˜ì„¸ìš”
- 2-3ê°œ ë¬¸ë‹¨ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ì„¸ìš”

ì›¹ì—ì„œ ìµœì‹  ì •ë³´ë¥¼ í™•ì¸í•˜ì—¬ ë‹µë³€ì„ ë³´ì™„í•´ì£¼ì„¸ìš”."""

        elif complexity == "moderate":
            # Moderate depth for technical or specific questions
            prompt = f"""ë‹¹ì‹ ì€ MSP ê¸°ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. '{best_match}'ì— ëŒ€í•œ ì§ˆë¬¸ "{question}"ì— ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

=== ë‚´ë¶€ í‰ê°€ ì •ë³´ ===
íšŒì‚¬: {best_match} (í‰ê°€ í‰ê· : {internal_avg}/5ì )
ê´€ë ¨ í‰ê°€ ë‚´ìš©:
{internal_context}

=== ë¶„ì„ í¬ì¸íŠ¸ ===
- ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰ëœ ê¸°ìˆ ì´ë‚˜ ìš”êµ¬ì‚¬í•­ì— ëŒ€í•œ êµ¬ì²´ì  ì—­ëŸ‰ í‰ê°€
- í•´ë‹¹ ë¶„ì•¼ì—ì„œì˜ ì‹¤ì œ í”„ë¡œì íŠ¸ ê²½í—˜ê³¼ ì‚¬ë¡€
- ê¸°ìˆ ì  ê°•ì ê³¼ ì œì•½ì‚¬í•­ì˜ ê· í˜•ì¡íŒ í‰ê°€
- ì‹¤ë¬´ì  ê´€ì ì—ì„œì˜ ì í•©ì„±ê³¼ ê³ ë ¤ì‚¬í•­

=== ì‘ë‹µ êµ¬ì¡° ===
**í•µì‹¬ ë‹µë³€**: [ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ ê²°ë¡ ]
**ì—­ëŸ‰ ë¶„ì„**: [êµ¬ì²´ì  ê¸°ìˆ  ì—­ëŸ‰ê³¼ ê²½í—˜]  
**ì‹¤ë¬´ ê³ ë ¤ì‚¬í•­**: [í˜‘ì—… ì‹œ ìœ ì˜ì ì´ë‚˜ ê¶Œì¥ì‚¬í•­]

ì›¹ì—ì„œ ê´€ë ¨ ê¸°ìˆ  ë™í–¥ê³¼ í”„ë¡œì íŠ¸ ì‚¬ë¡€ë¥¼ ì¡°ì‚¬í•˜ì—¬ í¬í•¨í•´ì£¼ì„¸ìš”."""

        else:  # complex
            # Full comprehensive analysis for complex questions
            prompt = f"""ë‹¹ì‹ ì€ í´ë¼ìš°ë“œ ë° MSP ì‚°ì—…ì˜ ì‹œë‹ˆì–´ ë¦¬ì„œì¹˜ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. '{best_match}'ì— ëŒ€í•œ ë³µí•© ì§ˆë¬¸ "{question}"ì— ë‹µë³€í•˜ê¸° ìœ„í•´, ë‚´ë¶€ í‰ê°€ ë°ì´í„°ì™€ ìµœì‹  ì›¹ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì „ë¬¸ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.

=== ë‚´ë¶€ í‰ê°€ ë°ì´í„° (ê¸°ì¤€ì ) ===
íšŒì‚¬ëª…: {best_match}
ë‚´ë¶€ í‰ê°€ í‰ê· : {internal_avg}/5ì  (ì´ {len(all_internal_scores)}ê°œ í‰ê°€ í•­ëª©)
í‰ê°€ ì¹´í…Œê³ ë¦¬: {len(category_data)}ê°œ ë¶„ì•¼

ì£¼ìš” ë‚´ë¶€ í‰ê°€ ë‚´ìš©:
{internal_context}

=== ì¢…í•© ë¶„ì„ ì§€ì¹¨ ===

1. **ì§ˆë¬¸ ìœ í˜•ë³„ í•„ìˆ˜ ìš”ì†Œ**
   - ë¹„êµ/ê²½ìŸ ì§ˆë¬¸ ("ëŒ€ë¹„", "ê°•ì ", "ì•½ì ", "vs") â†’ ë°˜ë“œì‹œ ê²½ìŸì‚¬ ë¶„ì„ê³¼ ìˆœìœ„ ë¹„êµ í¬í•¨
   - ê¸°ìˆ  ì—­ëŸ‰ ì§ˆë¬¸ (êµ¬ì²´ì  ê¸°ìˆ ëª… ì–¸ê¸‰) â†’ í•´ë‹¹ ê¸°ìˆ ì˜ ì§ì ‘ ê²½í—˜ê³¼ í”„ë¡œì íŠ¸ ì‚¬ë¡€ ê²€ì¦
   - ëŒ€ì•ˆ ìš”ì²­ ("ë” ì¢‹ì€", "ì˜µì…˜", "ì¶”ì²œ") â†’ êµ¬ì²´ì  ëŒ€ì•ˆ íšŒì‚¬ë“¤ê³¼ ì„ íƒ ê¸°ì¤€ ì œì‹œ
   - ì •ë¶€/ê³µê³µ ì–¸ê¸‰ â†’ ê³µê³µ í”„ë¡œì íŠ¸ ê²½í—˜ê³¼ ì •ë¶€ ê·œì • ì¤€ìˆ˜ ëŠ¥ë ¥ í‰ê°€

2. **ì¦ê±° ê¸°ë°˜ ë¶„ì„ ì›ì¹™**
   - êµ¬ì²´ì  ê¸°ìˆ ëª…ì´ ì–¸ê¸‰ë˜ë©´ í•´ë‹¹ ê¸°ìˆ ì˜ ì‹¤ì œ êµ¬í˜„ ê²½í—˜ë§Œ ì¸ì •
   - ì¼ë°˜ì  AI ì—­ëŸ‰ì„ íŠ¹ìˆ˜ ê¸°ìˆ  ì „ë¬¸ì„±ìœ¼ë¡œ í™•ëŒ€ í•´ì„ ê¸ˆì§€
   - ì ìˆ˜ë§Œìœ¼ë¡œ íŒë‹¨í•˜ì§€ ë§ê³  ë‹µë³€ ë‚´ìš©ì˜ êµ¬ì²´ì„±ê³¼ ê´€ë ¨ì„± ìš°ì„  í‰ê°€
   - ê²½ìŸì‚¬ ë¹„êµ ì‹œ ë™ì¼í•œ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ë¥¸ íšŒì‚¬ë“¤ì˜ ì—­ëŸ‰ë„ ê²€í† 

=== ì‘ë‹µ í•„ìˆ˜ êµ¬ì„± ìš”ì†Œ ===

**ğŸ’¡ ì§ì ‘ ë‹µë³€**
[ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ - "ì˜ˆ/ì•„ë‹ˆì˜¤" ë˜ëŠ” "ì í•©/ë¶€ì í•©" ë“± ëª…í™•í•œ ê²°ë¡  í¬í•¨]

**ğŸ” ìƒì„¸ ë¶„ì„**
- **{best_match} ì—­ëŸ‰ í‰ê°€**: [í•´ë‹¹ ë¶„ì•¼ êµ¬ì²´ì  ê²½í—˜ê³¼ í”„ë¡œì íŠ¸ ì‚¬ë¡€, ì ìˆ˜ ê·¼ê±°]
- **ê¸°ìˆ ì  ì í•©ì„±**: [ì–¸ê¸‰ëœ ê¸°ìˆ ë“¤ì— ëŒ€í•œ ê°œë³„ ì—­ëŸ‰ í‰ê°€]
- **ìµœì‹  ë™í–¥**: [ì›¹ì—ì„œ í™•ì¸ëœ ìµœê·¼ í”„ë¡œì íŠ¸ë‚˜ ê¸°ìˆ  ë°œì „ ì‚¬í•­]

**âš–ï¸ ê²½ìŸ ë¹„êµ (ë¹„êµ ì§ˆë¬¸ì¸ ê²½ìš° í•„ìˆ˜)**
- **ì§ì ‘ ê²½ìŸì‚¬**: [ìœ ì‚¬í•œ ê¸°ìˆ  ì—­ëŸ‰ì„ ê°€ì§„ ë‹¤ë¥¸ MSPë“¤ê³¼ì˜ ë¹„êµ]
- **ìš°ìœ„ ìš”ì†Œ**: [{best_match}ê°€ ê²½ìŸì‚¬ ëŒ€ë¹„ ì•ì„œëŠ” êµ¬ì²´ì  ë¶€ë¶„]
- **ì—´ìœ„ ìš”ì†Œ**: [ê²½ìŸì‚¬ê°€ ë” ë‚˜ì€ êµ¬ì²´ì  ë¶€ë¶„]
- **ëŒ€ì•ˆ ì¶”ì²œ**: [ë” ì í•©í•œ íšŒì‚¬ê°€ ìˆë‹¤ë©´ êµ¬ì²´ì  ì´ìœ ì™€ í•¨ê»˜ ì œì‹œ]

**ğŸ¯ ì‹¤ë¬´ ê¶Œì¥ì‚¬í•­**
- **ì„ íƒ ê¸°ì¤€**: [í•´ë‹¹ í”„ë¡œì íŠ¸ì—ì„œ {best_match}ë¥¼ ì„ íƒí•´ì•¼ í•˜ëŠ” êµ¬ì²´ì  ì¡°ê±´]
- **ì£¼ì˜ì‚¬í•­**: [í˜‘ì—… ì‹œ ë°˜ë“œì‹œ í™•ì¸í•´ì•¼ í•  ì œì•½ ìš”ì†Œë‚˜ ë¦¬ìŠ¤í¬]
- **ì„±ê³µ ìš”ì¸**: [í”„ë¡œì íŠ¸ ì„±ê³µì„ ìœ„í•´ í•„ìš”í•œ ì¶”ê°€ ì¡°ê±´ì´ë‚˜ ì§€ì›]

=== í’ˆì§ˆ ì²´í¬ë¦¬ìŠ¤íŠ¸ ===
âœ… ì§ˆë¬¸ì—ì„œ ìš”êµ¬í•œ ëª¨ë“  ìš”ì†Œì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€í–ˆëŠ”ê°€?
âœ… ë¹„êµ ìš”ì²­ ì‹œ ì‹¤ì œ ê²½ìŸì‚¬ì™€ ë¹„êµ ë¶„ì„ì„ ì œê³µí–ˆëŠ”ê°€?
âœ… ê¸°ìˆ  ì—­ëŸ‰ í‰ê°€ ì‹œ í•´ë‹¹ ê¸°ìˆ ì˜ ì§ì ‘ ê²½í—˜ì„ í™•ì¸í–ˆëŠ”ê°€?
âœ… ëŒ€ì•ˆ ìš”ì²­ ì‹œ ë” ë‚˜ì€ ì„ íƒì§€ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì œì‹œí–ˆëŠ”ê°€?
âœ… ì¼ë°˜ë¡ ì´ ì•„ë‹Œ êµ¬ì²´ì  ê·¼ê±°ì™€ ì‚¬ë¡€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê²°ë¡ ì„ ë„ì¶œí–ˆëŠ”ê°€?

ìœ„ ì§€ì¹¨ì— ë”°ë¼ ì¢…í•©ì ì´ê³  ì „ë¬¸ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”."""

    except Exception as e:
        traceback.print_exc()
        return {"answer": f"ë‚´ë¶€ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", "advanced": True}

    # Enhanced Perplexity API call
    try:
        response = requests.post(
            "https://api.perplexity.ai/chat/completions",
            headers={
                "Authorization": f"Bearer {os.getenv('PPLX_API_KEY')}",
                "Content-Type": "application/json",
            },
            json={
                "model": "sonar",
                "messages": [
                    {
                        "role": "system", 
                        "content": "ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ í´ë¼ìš°ë“œ ë° MSP ì‚°ì—… ì „ë¬¸ ë¦¬ì„œì¹˜ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ë‚´ë¶€ í‰ê°€ ë°ì´í„°ì™€ ìµœì‹  ì›¹ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸í…”ë¦¬ì „ìŠ¤ë¥¼ ì œê³µí•˜ë©°, íŒ©íŠ¸ì— ê¸°ë°˜í•œ ê°ê´€ì  ë¶„ì„ê³¼ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¤‘ì‹œí•©ë‹ˆë‹¤."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "temperature": 0.2,  # Lower for more factual analysis
                "max_tokens": 1500,  # Increased for comprehensive analysis
                "top_p": 0.8
            },
            timeout=45  # Increased timeout for comprehensive web search
        )
        
        print(f"ğŸ” Perplexity API status: {response.status_code}")
        
        if response.status_code == 200:
            import re
            result = response.json()
            answer = result["choices"][0]["message"]["content"].strip()
            
            # Enhanced post-processing for professional consistency
            # Remove citation markers that might interfere with readability
            answer = re.sub(r"\[\d+\]", "", answer)
            
            # Fix common terminology
            professional_fixes = {
                "ì„¤ë£¨ì…˜": "ì†”ë£¨ì…˜",
                "í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤": "í´ë¼ìš°ë“œ ì†”ë£¨ì…˜",
                "ë¹…ë°ì´í„°": "ë¹…ë°ì´í„°",
                "ë¨¸ì‹ ëŸ¬ë‹": "ë¨¸ì‹ ëŸ¬ë‹",
                "ë”¥ëŸ¬ë‹": "ë”¥ëŸ¬ë‹",
                "ì¸ê³µì§€ëŠ¥": "AI"
            }
            
            for old_term, new_term in professional_fixes.items():
                answer = answer.replace(old_term, new_term)
            
            # Clean up formatting
            answer = re.sub(r"\n{3,}", "\n\n", answer)  # Remove excessive line breaks
            answer = answer.strip()
            
            return {
                "answer": answer, 
                "advanced": True, 
                "evidence": internal_chunks,
                "model_used": "perplexity-sonar-enhanced",
                "data_integration": {
                    "internal_data_points": len(internal_chunks),
                    "internal_average": internal_avg,
                    "categories_covered": len(category_data),
                    "web_enhanced": True
                },
                "analysis_type": "comprehensive_web_intelligence"
            }
        else:
            # Fallback to internal data analysis if Perplexity fails
            print(f"âŒ Perplexity API failed: {response.status_code}, falling back to internal analysis")
            
            if internal_chunks:
                fallback_answer = f"""**[ë‚´ë¶€ ë°ì´í„° ê¸°ë°˜ ë¶„ì„]**

**í•µì‹¬ ë‹µë³€**
{best_match}ì— ëŒ€í•œ ì§ˆë¬¸ "{question}"ì— ëŒ€í•´ ë‚´ë¶€ í‰ê°€ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•œ ê²°ê³¼ì…ë‹ˆë‹¤.

**í‰ê°€ í˜„í™©**
- ë‚´ë¶€ í‰ê°€ í‰ê· : {internal_avg}/5ì 
- í‰ê°€ í•­ëª© ìˆ˜: {len(all_internal_scores)}ê°œ
- í‰ê°€ ì¹´í…Œê³ ë¦¬: {len(category_data)}ê°œ ë¶„ì•¼

**ì£¼ìš” í‰ê°€ ë‚´ìš©**
{chr(10).join([f"â€¢ [{chunk['category']}] {chunk['question'][:60]}{'...' if len(chunk['question']) > 60 else ''} (ì ìˆ˜: {chunk['score']}/5)" for chunk in internal_chunks[:5]])}

**ì œí•œì‚¬í•­**
ì™¸ë¶€ ì›¹ ì •ë³´ ì—°ë™ì— ì‹¤íŒ¨í•˜ì—¬ ë‚´ë¶€ í‰ê°€ ë°ì´í„°ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì œí•œì  ë¶„ì„ì…ë‹ˆë‹¤. ìµœì‹  ì •ë³´ë‚˜ ì‹œì¥ ë™í–¥ì€ ë³„ë„ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."""
                
                return {
                    "answer": fallback_answer,
                    "advanced": True,
                    "evidence": internal_chunks,
                    "model_used": "internal-fallback",
                    "data_integration": {
                        "internal_data_points": len(internal_chunks),
                        "internal_average": internal_avg,
                        "web_enhanced": False,
                        "fallback_reason": "perplexity_api_failure"
                    }
                }
            else:
                return {
                    "answer": f"{best_match}ì— ëŒ€í•œ ì¶©ë¶„í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", 
                    "advanced": True
                }
                
    except Exception as e:
        traceback.print_exc()
        return {
            "answer": f"ì›¹ ê¸°ë°˜ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}", 
            "advanced": True
        }

def extract_msp_name(question: str) -> str:
    from openai import OpenAI
    import os

    CLOVA_API_KEY = os.getenv("CLOVA_API_KEY_OPENAI")
    API_URL = "https://clovastudio.stream.ntruss.com/v1/openai"
    client = OpenAI(api_key=CLOVA_API_KEY, base_url=API_URL)
    model = "HCX-005"

    prompt = (
        f"ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ì‹¤ì œ í´ë¼ìš°ë“œ MSP íŒŒíŠ¸ë„ˆì‚¬ì˜ ì´ë¦„ë§Œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì„¸ìš”. ë¬¸ì¥ ì „ì²´ë¥¼ ì¶œë ¥í•˜ì§€ ë§ê³ , íšŒì‚¬ëª…ë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n"
        f"[ì˜ˆì‹œ]\n"
        f"ì§ˆë¬¸: 'ITCEN CLOITì— ëŒ€í•´ ì•Œë ¤ì¤˜'\nì‘ë‹µ: ITCEN CLOIT\n"
        f"ì§ˆë¬¸: 'Lominì˜ AI ì—­ëŸ‰ì€?'\nì‘ë‹µ: Lomin\n"
        f"ì§ˆë¬¸: 'ë² ìŠ¤í•€ê¸€ë¡œë²Œì˜ MLOps ì‚¬ë¡€ëŠ”?'\nì‘ë‹µ: ë² ìŠ¤í•€ê¸€ë¡œë²Œ\n"
        f"ì§ˆë¬¸: '{question}'\n"
        f"ì‘ë‹µ:"
    )

    try:
        clova_response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ì§ˆë¬¸ì—ì„œ í´ë¼ìš°ë“œ MSP íšŒì‚¬ ì´ë¦„ë§Œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•´ ì£¼ì„¸ìš”. ë¬¸ì¥ì€ ì ˆëŒ€ ì‘ì„±í•˜ì§€ ë§ê³ , íšŒì‚¬ëª…ë§Œ ë‹¨ë…ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”. ì˜ˆ: ë² ìŠ¤í•€ê¸€ë¡œë²Œ"},
                {"role": "user", "content": prompt}
            ],
            top_p=0.6,
            temperature=0.3,
            max_tokens=20
        )
        raw = clova_response.choices[0].message.content.strip()
        print(f"ğŸ” Extracted raw MSP name: {raw}")
        return raw
    except Exception as e:
        print(f"âŒ Error extracting MSP name: {e}")
        return ""

def run_msp_news_summary_clova(question: str):
    import urllib.parse
    import urllib.request
    import traceback

    msp_name = extract_msp_name(question)
    if not msp_name:
        return {"answer": "íšŒì‚¬ëª…ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.", "advanced": True}

    # Get vector DB information for the MSP
    try:
        query_vector = query_embed(question)
        query_results = collection.query(
            query_embeddings=[query_vector],
            n_results=10
        )
        db_chunks = [
            f"Q: {chunk['question']}\nA: {chunk['answer']}"
            for chunk in query_results["metadatas"][0]
            if chunk.get("msp_name") == msp_name and chunk.get("question") and chunk.get("answer")
        ][:5]
        db_context = "\n\n".join(db_chunks)
    except Exception as e:
        db_context = ""

    try:
        query = urllib.parse.quote(msp_name)
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=10&sort=sim"
        headers = {
            "X-Naver-Client-Id": os.getenv("NAVER_CLIENT_ID"),
            "X-Naver-Client-Secret": os.getenv("NAVER_CLIENT_SECRET")
        }
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req) as response:
            if response.status != 200:
                raise Exception(f"Naver API Error: {response.status}")
            news_data = json.loads(response.read().decode("utf-8"))

        url_web = f"https://openapi.naver.com/v1/search/webkr.json?query={query}&display=3&sort=sim"
        req_web = urllib.request.Request(url_web, headers=headers)
        with urllib.request.urlopen(req_web) as response_web:
            if response_web.status != 200:
                raise Exception(f"Naver Web API Error: {response_web.status}")
            web_data = json.loads(response_web.read().decode("utf-8"))

        if "items" not in news_data or not news_data["items"]:
            return {"answer": f"{msp_name}ì— ëŒ€í•œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "advanced": True}

        article_summaries = "\n".join(
            f"- ì œëª©: {item['title'].replace('<b>', '').replace('</b>', '')}\n  ìš”ì•½: {item['description'].replace('<b>', '').replace('</b>', '')}"
            for item in news_data["items"]
        )

        web_summaries = "\n".join(
            f"- ì œëª©: {item['title'].replace('<b>', '').replace('</b>', '')}\n  ìš”ì•½: {item['description'].replace('<b>', '').replace('</b>', '')}"
            for item in web_data.get("items", [])
        )

        prompt = (
            f"ë‹¤ìŒì€ í´ë¼ìš°ë“œ MSP ê¸°ì—… '{msp_name}'ì— ëŒ€í•œ ë‰´ìŠ¤ ê¸°ì‚¬, ì›¹ ë¬¸ì„œ, ì¸í„°ë·° Q&A ìš”ì•½ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì‘ë‹µí•´ ì£¼ì„¸ìš”.\n"
            f"ì‚¬ìš©ì ì§ˆë¬¸: \"{question}\"\n\n"
            f"[DB ê¸°ë°˜ ì •ë³´]\n{db_context}\n\n"
            f"[ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½]\n{article_summaries}\n\n"
            f"[ì›¹ ë¬¸ì„œ ìš”ì•½]\n{web_summaries}\n\n"
            f"[ì‘ë‹µ ì§€ì¹¨]\n"
            f"- ê¸°ì‚¬, ì›¹ ë¬¸ì„œ, ì¸í„°ë·° Q&A ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš”.\n"
            f"- ì—†ëŠ” ì •ë³´ë¥¼ ê¾¸ë©°ë‚´ê±°ë‚˜ ì¶”ë¡ í•˜ì§€ ë§ˆì„¸ìš”.\n"
            f"- ê¸°ì—…ì˜ ìˆ˜ìƒ ì‹¤ì , í˜‘ì—…, íˆ¬ì, ì¸ë ¥ êµ¬ì„± ë“± í•µì‹¬ ì •ë³´ë¥¼ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”."
        )

        CLOVA_API_KEY = os.getenv("CLOVA_API_KEY_OPENAI")
        API_URL = "https://clovastudio.stream.ntruss.com/v1/openai"
        from openai import OpenAI
        client = OpenAI(api_key=CLOVA_API_KEY, base_url=API_URL)
        model = "HCX-005"

        clova_response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "ì •í™•í•˜ê³  ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì‘ë‹µì„ ìì—°ìŠ¤ëŸ½ê³  ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”."},
                {"role": "user", "content": prompt}
            ],
            top_p=0.6,
            temperature=0.3,
            max_tokens=500
        )
        answer = clova_response.choices[0].message.content.strip()
        answer = answer.replace("ì„¤ë£¨ì…˜", "ì†”ë£¨ì…˜")
        return {"answer": answer, "advanced": True, "evidence": news_data["items"], "web_evidence": web_data.get("items", [])}
    except Exception as e:
        traceback.print_exc()
        return {"answer": f"ë‰´ìŠ¤ ê¸°ë°˜ ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}", "advanced": True}

def run_msp_news_summary_claude(question: str):
    """
    Enhanced version with more data for Claude
    """
    import urllib.parse
    import urllib.request
    import traceback
    import anthropic
    import os

    msp_name = extract_msp_name(question)
    if not msp_name:
        return {"answer": "íšŒì‚¬ëª…ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.", "advanced": True}

    # Enhanced vector DB search - get more relevant data for Claude
    try:
        query_vector = query_embed(question)
        query_results = collection.query(
            query_embeddings=[query_vector],
            n_results=15
        )
        db_chunks = [
            f"Q: {chunk['question']}\nA: {chunk['answer']} (ì ìˆ˜: {chunk.get('score', 'N/A')}/5)"
            for chunk in query_results["metadatas"][0]
            if chunk.get("msp_name") == msp_name and chunk.get("question") and chunk.get("answer")
        ][:8]
        db_context = "\n\n".join(db_chunks)
    except Exception as e:
        db_context = ""

    try:
        # Enhanced API calls - get more comprehensive data
        query = urllib.parse.quote(msp_name)
        
        # Get more news articles for better coverage
        url = f"https://openapi.naver.com/v1/search/news.json?query={query}&display=15&sort=sim"
        headers = {
            "X-Naver-Client-Id": os.getenv("NAVER_CLIENT_ID"),
            "X-Naver-Client-Secret": os.getenv("NAVER_CLIENT_SECRET")
        }
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req) as response:
            if response.status != 200:
                raise Exception(f"Naver API Error: {response.status}")
            news_data = json.loads(response.read().decode("utf-8"))

        # Get more web documents for comprehensive view
        url_web = f"https://openapi.naver.com/v1/search/webkr.json?query={query}&display=7&sort=sim"
        req_web = urllib.request.Request(url_web, headers=headers)
        with urllib.request.urlopen(req_web) as response_web:
            if response_web.status != 200:
                raise Exception(f"Naver Web API Error: {response_web.status}")
            web_data = json.loads(response_web.read().decode("utf-8"))

        if "items" not in news_data or not news_data["items"]:
            return {"answer": f"{msp_name}ì— ëŒ€í•œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "advanced": True}

        # Enhanced data cleaning and formatting for Claude
        def clean_text(text):
            return text.replace('<b>', '').replace('</b>', '').replace('&quot;', '"').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')

        # Structured news formatting with more metadata
        news_items = []
        for i, item in enumerate(news_data["items"][:12], 1):  # Top 12 most relevant
            title = clean_text(item.get('title', ''))
            desc = clean_text(item.get('description', ''))
            pub_date = item.get('pubDate', '')[:10] if item.get('pubDate') else 'N/A'
            
            if title and desc:  # Only include substantial content
                news_items.append(f"{i}. [{pub_date}] {title}\n   ì„¸ë¶€ë‚´ìš©: {desc}")

        # Structured web formatting with quality filtering
        web_items = []
        for i, item in enumerate(web_data.get("items", [])[:5], 1):  # Top 5 web docs
            title = clean_text(item.get('title', ''))
            desc = clean_text(item.get('description', ''))
            
            if title and desc and len(desc) > 50:  # Filter for substantial content
                web_items.append(f"{i}. {title}\n   ìš”ì•½: {desc}")

        article_text = "\n\n".join(news_items)
        web_text = "\n\n".join(web_items)

        # Enhanced prompt designed for Claude's analytical capabilities
        prompt = f"""ë‹¤ìŒì€ í´ë¼ìš°ë“œ MSP íŒŒíŠ¸ë„ˆì‚¬ '{msp_name}'ì— ëŒ€í•œ ì¢…í•© ì •ë³´ì…ë‹ˆë‹¤. ì´ ë‹¤ì–‘í•œ ì •ë³´ì›ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ì „ë¬¸ì ì´ê³  í†µì°°ë ¥ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{question}"

=== ë‚´ë¶€ í‰ê°€ ë°ì´í„° (ê°€ì¥ ì‹ ë¢°ë„ ë†’ìŒ) ===
{db_context}

=== ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ ({len(news_items)}ê°œ ìµœì‹  ê¸°ì‚¬) ===
{article_text}

=== ì›¹ ë¬¸ì„œ ì •ë³´ ({len(web_items)}ê°œ ê´€ë ¨ ë¬¸ì„œ) ===
{web_text}

=== ì „ë¬¸ê°€ ìˆ˜ì¤€ ë¶„ì„ ì§€ì¹¨ ===
1. **ì •ë³´ í†µí•© ë¶„ì„**: ë‚´ë¶€ í‰ê°€, ë‰´ìŠ¤, ì›¹ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ê· í˜•ì¡íŒ ì‹œê° ì œê³µ
2. **ì‹ ë¢°ë„ ìš°ì„ ìˆœìœ„**: ë‚´ë¶€ í‰ê°€ ë°ì´í„° â†’ ê³µì‹ ë‰´ìŠ¤ â†’ ì›¹ ë¬¸ì„œ ìˆœìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì ìš©
3. **êµ¬ì²´ì  ê·¼ê±° ì œì‹œ**: 
   - í‰ê°€ ì ìˆ˜ë‚˜ êµ¬ì²´ì  ìˆ˜ì¹˜ ìš°ì„  ì–¸ê¸‰
   - ì‹œê¸°ë³„ ë³€í™”ë‚˜ ìµœê·¼ ë™í–¥ íŒŒì•…
   - ê²½ìŸì‚¬ ëŒ€ë¹„ ì°¨ë³„í™” ìš”ì†Œ ì‹ë³„
4. **ì‹¤ë¬´ì  ê´€ì **: ì‹¤ì œ ê³ ê°/íŒŒíŠ¸ë„ˆ ê´€ì ì—ì„œ ì˜ë¯¸ìˆëŠ” ì •ë³´ ìš°ì„  ì •ë¦¬
5. **ê°ê´€ì  ê· í˜•**: ê°•ì ê³¼ ê°œì„ ì˜ì—­ì„ ëª¨ë‘ ê³ ë ¤í•œ ê³µì •í•œ í‰ê°€

ì‘ë‹µ í˜•ì‹: ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë˜, ë§ˆì¼€íŒ… í‘œí˜„ë³´ë‹¤ëŠ” íŒ©íŠ¸ì™€ ë°ì´í„° ì¤‘ì‹¬ìœ¼ë¡œ ì„œìˆ í•´ì£¼ì„¸ìš”."""

    except Exception as e:
        traceback.print_exc()
        return {"answer": f"ë‰´ìŠ¤ ê¸°ë°˜ ìš”ì•½ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}", "advanced": True}

    # Enhanced Claude API call with optimized parameters
    try:
        client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
        
        response = client.messages.create(
            model="claude-3-haiku-20240307",
            max_tokens=1500,
            temperature=0.2,  # Lower for more factual, analytical responses
            system="ë‹¹ì‹ ì€ 10ë…„ ì´ìƒ ê²½ë ¥ì˜ í´ë¼ìš°ë“œ ë° MSP ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì •ë³´ì›ì„ ì¢…í•© ë¶„ì„í•˜ì—¬ ê°ê´€ì ì´ê³  ì‹¤ìš©ì ì¸ í†µì°°ì„ ì œê³µí•˜ë©°, êµ¬ì²´ì  ê·¼ê±°ì™€ ë°ì´í„°ì— ê¸°ë°˜í•œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ í‰ê°€ë¥¼ ì¤‘ì‹œí•©ë‹ˆë‹¤.",
            messages=[{
                "role": "user", 
                "content": prompt
            }]
        )
        
        answer = response.content[0].text.strip()
        
        # Enhanced post-processing for professional consistency
        answer = answer.replace("ì„¤ë£¨ì…˜", "ì†”ë£¨ì…˜")
        answer = answer.replace("í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤", "í´ë¼ìš°ë“œ ì†”ë£¨ì…˜")
        
        return {
            "answer": answer, 
            "advanced": True, 
            "evidence": news_data["items"][:12], 
            "web_evidence": web_data.get("items", [])[:5],
            "model_used": "claude-3-haiku-enhanced",
            "data_summary": {
                "news_articles": len(news_items),
                "web_documents": len(web_items),
                "internal_qa_pairs": len(db_chunks),
                "total_sources": len(news_items) + len(web_items) + len(db_chunks)
            }
        }
        
    except Exception as e:
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Claude API error: {str(e)}")
